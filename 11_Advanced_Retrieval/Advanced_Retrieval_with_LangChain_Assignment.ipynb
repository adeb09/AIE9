{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-IqJAMkwnCF"
   },
   "source": [
    "# Session 11: Advanced Retrieval with LangChain\n",
    "\n",
    "## Learning Objectives:\n",
    "\n",
    "- Understand and implement multiple retrieval strategies for RAG\n",
    "- Compare naive, BM25, multi-query, parent-document, contextual compression, ensemble, and semantic chunking approaches\n",
    "- Build RAG chains over a health and wellness knowledge base using LangChain and QDrant\n",
    "\n",
    "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
    "\n",
    "We'll touch on:\n",
    "\n",
    "- Naive Retrieval\n",
    "- Best-Matching 25 (BM25)\n",
    "- Multi-Query Retrieval\n",
    "- Parent-Document Retrieval\n",
    "- Contextual Compression (a.k.a. Rerank)\n",
    "- Ensemble Retrieval\n",
    "- Semantic chunking\n",
    "\n",
    "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
    "\n",
    "There will be two breakout rooms:\n",
    "\n",
    "- ðŸ¤ Breakout Room Part #1\n",
    "  - Task 1: Getting Dependencies!\n",
    "  - Task 2: Data Collection and Preparation\n",
    "  - Task 3: Setting Up QDrant!\n",
    "  - Task 4-10: Retrieval Strategies\n",
    "- ðŸ¤ Breakout Room Part #2\n",
    "  - Activity: Evaluate with Ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rKP3hgHivpe"
   },
   "source": [
    "---\n",
    "\n",
    "# ðŸ¤ Breakout Room Part #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xes8oT-xHN7"
   },
   "source": [
    "## Task 1: Getting Dependencies!\n",
    "\n",
    "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7OHJXzfyJyA"
   },
   "source": [
    "We'll also provide our OpenAI key, as well as our Cohere API key.\n",
    "\n",
    "> NOTE: Create a `.env` file in this directory with `OPENAI_API_KEY` and `COHERE_API_KEY` to avoid being prompted each time."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7LttlDQUYgSI",
    "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54",
    "ExecuteTime": {
     "end_time": "2026-02-19T07:01:10.609852Z",
     "start_time": "2026-02-19T07:01:10.594303Z"
    }
   },
   "source": [
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "from langsmith import EvaluationResult\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3iUahNiJyQbv",
    "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b",
    "ExecuteTime": {
     "end_time": "2026-02-19T07:01:12.647103Z",
     "start_time": "2026-02-19T07:01:12.619005Z"
    }
   },
   "source": [
    "if not os.environ.get(\"COHERE_API_KEY\"):\n",
    "    os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mw304iAFyRtl"
   },
   "source": [
    "## Task 2: Data Collection and Preparation\n",
    "\n",
    "We'll be using our Health and Wellness Guide - a comprehensive resource covering exercise, nutrition, sleep, stress management, habits, and common health concerns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A92NC2QZzCsi"
   },
   "source": [
    "### Data Preparation\n",
    "\n",
    "We'll load the wellness guide as a single document, then split it into smaller chunks using a `RecursiveCharacterTextSplitter` for our vector store. We also keep the raw (unsplit) document for use with the Parent Document Retriever and Semantic Chunker later."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GshBjVRJZ6p8",
    "ExecuteTime": {
     "end_time": "2026-02-19T07:12:38.367341Z",
     "start_time": "2026-02-19T07:12:37.564799Z"
    }
   },
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = TextLoader(\"data/HealthWellnessGuide.txt\")\n",
    "raw_docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "wellness_docs = text_splitter.split_documents(raw_docs)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gQphb6y0C0S"
   },
   "source": [
    "Let's verify our data was loaded and split correctly!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PkUkCf7DaMiq",
    "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199",
    "ExecuteTime": {
     "end_time": "2026-02-19T07:12:42.561917Z",
     "start_time": "2026-02-19T07:12:42.511183Z"
    }
   },
   "source": [
    "print(f\"Raw documents: {len(raw_docs)}\")\n",
    "print(f\"Split chunks: {len(wellness_docs)}\")\n",
    "print(f\"\\nExample chunk:\\n{wellness_docs[0]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw documents: 1\n",
      "Split chunks: 45\n",
      "\n",
      "Example chunk:\n",
      "page_content='The Personal Wellness Guide\n",
      "A Comprehensive Resource for Health and Well-being\n",
      "\n",
      "PART 1: EXERCISE AND MOVEMENT\n",
      "\n",
      "Chapter 1: Understanding Exercise Basics\n",
      "\n",
      "Exercise is one of the most important things you can do for your health. Regular physical activity can improve your brain health, help manage weight, reduce the risk of disease, strengthen bones and muscles, and improve your ability to do everyday activities.' metadata={'source': 'data/HealthWellnessGuide.txt'}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWaQpdHl0Gzc"
   },
   "source": [
    "## Task 3: Setting up QDrant!\n",
    "\n",
    "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"wellness_guide\".\n",
    "\n",
    "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
    "\n",
    "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NT8ihRJbYmMT",
    "ExecuteTime": {
     "end_time": "2026-02-19T07:13:14.241995Z",
     "start_time": "2026-02-19T07:13:08.547260Z"
    }
   },
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "vectorstore = QdrantVectorStore.from_documents(\n",
    "    wellness_docs,\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"wellness_guide\",\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-x2SS4Rh0hiN"
   },
   "source": [
    "## Task 4: Naive RAG Chain\n",
    "\n",
    "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEH7X5Ai08FH"
   },
   "source": [
    "### R - Retrieval\n",
    "\n",
    "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
    "\n",
    "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GFDPrNBtb72o",
    "ExecuteTime": {
     "end_time": "2026-02-19T07:14:15.530329Z",
     "start_time": "2026-02-19T07:14:15.516856Z"
    }
   },
   "source": [
    "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbBhyQjz06dx"
   },
   "source": [
    "### A - Augmented\n",
    "\n",
    "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7uSz-Dbqcoki",
    "ExecuteTime": {
     "end_time": "2026-02-19T07:14:32.872890Z",
     "start_time": "2026-02-19T07:14:32.810271Z"
    }
   },
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"\\\n",
    "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
    "\n",
    "If you do not know the answer, or are unsure, say you don't know.\n",
    "\n",
    "Query:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlRzpb231GGJ"
   },
   "source": [
    "### G - Generation\n",
    "\n",
    "We're going to leverage `gpt-4.1-nano` as our LLM today, as - again - we want this to largely be about the Retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "c-1t9H60dJLg",
    "ExecuteTime": {
     "end_time": "2026-02-19T07:14:47.275013Z",
     "start_time": "2026-02-19T07:14:47.232751Z"
    }
   },
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mg3QRGzA1M2x"
   },
   "source": [
    "### LCEL RAG Chain\n",
    "\n",
    "We're going to use LCEL to construct our chain.\n",
    "\n",
    "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0bvstS7mdOW3",
    "ExecuteTime": {
     "end_time": "2026-02-19T07:18:29.826182Z",
     "start_time": "2026-02-19T07:18:29.800812Z"
    }
   },
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "naive_retrieval_chain = (\n",
    "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
    "    # \"question\" : populated by getting the value of the \"question\" key\n",
    "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
    "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
    "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
    "    #              by getting the value of the \"context\" key from the previous step\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
    "    #              into the LLM and stored in a key called \"response\"\n",
    "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izKujhNb1ZG8"
   },
   "source": [
    "Let's see how this simple chain does on a few different prompts.\n",
    "\n",
    "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "LI-5ueEddku9",
    "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e",
    "ExecuteTime": {
     "end_time": "2026-02-19T07:18:51.754228Z",
     "start_time": "2026-02-19T07:18:46.811413Z"
    }
   },
   "source": [
    "naive_retrieval_chain.invoke({\"question\" : \"What exercises can help with lower back pain?\"})[\"response\"].content"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Exercises that can help with lower back pain include:\\n\\n- **Cat-Cow Stretch:** Start on hands and knees, alternate between arching your back up (cat) and letting it sag down (cow). Do 10-15 repetitions.\\n\\n- **Bird Dog:** From hands and knees, extend opposite arm and leg while keeping your core engaged. Hold for 5 seconds, then switch sides. Do 10 repetitions per side.\\n\\n- **Pelvic Tilts:** Lie on your back with knees bent, flatten your back against the floor by tightening your abs and tilting the pelvis up slightly. Hold for 10 seconds and repeat 8-12 times.\\n\\n- **Partial Crunches:** Lie on your back with knees bent, cross arms over chest, tighten stomach muscles, and raise shoulders off the floor. Hold briefly, then lower. Do 8-12 repetitions.\\n\\n- **Knee-to-Chest Stretch:** Lie on your back, pull one knee toward your chest while keeping the other foot flat. Hold for 15-30 seconds, then switch legs.\\n\\nThese exercises, performed gently and regularly, can help alleviate lower back pain and improve strength and flexibility.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "43zdcdUydtXh",
    "outputId": "db874e67-f568-4ed1-b863-b7c17b387052",
    "ExecuteTime": {
     "end_time": "2026-02-19T07:19:07.955866Z",
     "start_time": "2026-02-19T07:19:04.959798Z"
    }
   },
   "source": [
    "naive_retrieval_chain.invoke({\"question\" : \"How does sleep affect overall health?\"})[\"response\"].content"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sleep has a significant impact on overall health. Adequate sleep (7-9 hours per night) is essential for physical health, mental well-being, and cognitive function. During sleep, the body repairs tissues, consolidates memories, and releases hormones that help regulate growth and appetite. Good sleep supports a healthy immune system, enhances mood, improves concentration, and contributes to better recovery from illnesses. Conversely, poor sleep or sleep disorders like insomnia can negatively affect these areas, highlighting the importance of maintaining good sleep hygiene and creating an optimal sleep environment for overall wellness.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "lpG6rlvvvKFq",
    "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5",
    "ExecuteTime": {
     "end_time": "2026-02-19T07:19:37.199490Z",
     "start_time": "2026-02-19T07:19:34.461175Z"
    }
   },
   "source": [
    "naive_retrieval_chain.invoke({\"question\" : \"What are some natural remedies for stress and headaches?\"})[\"response\"].content"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some natural remedies for stress and headaches include:\\n\\n- Drinking water and staying hydrated\\n- Applying cold or warm compresses to the head or neck\\n- Resting in a dark, quiet room\\n- Gentle massage of the temples and neck\\n- Using essential oils such as peppermint or lavender\\n- Taking small amounts of caffeine (with caution)\\n- Practicing relaxation techniques like deep breathing, progressive muscle relaxation, or grounding exercises\\n- Taking short walks, especially in nature\\n- Listening to calming music\\n\\nThese methods can help alleviate stress and manage headache symptoms naturally.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsbfQmbr1leg"
   },
   "source": [
    "Overall, this is not bad! Let's see if we can make it better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ft1vt8HPR16w"
   },
   "source": [
    "## Task 5: Best-Matching 25 (BM25) Retriever\n",
    "\n",
    "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
    "\n",
    "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
    "\n",
    "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qdF4wuj5R-cG",
    "ExecuteTime": {
     "end_time": "2026-02-19T07:20:13.618820Z",
     "start_time": "2026-02-19T07:20:13.594533Z"
    }
   },
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(wellness_docs)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIjJlBQ8drKH"
   },
   "source": [
    "We'll construct the same chain - only changing the retriever."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WR15EQG7SLuw",
    "ExecuteTime": {
     "end_time": "2026-02-19T07:21:08.671803Z",
     "start_time": "2026-02-19T07:21:08.646404Z"
    }
   },
   "source": [
    "bm25_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Gi-yXCDdvJk"
   },
   "source": [
    "Let's look at the responses!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "oY9qzmm3SOrF",
    "outputId": "4d4f450f-5978-460f-f242-b32407868353",
    "ExecuteTime": {
     "end_time": "2026-02-19T07:21:14.935679Z",
     "start_time": "2026-02-19T07:21:12.736133Z"
    }
   },
   "source": [
    "bm25_retrieval_chain.invoke({\"question\" : \"What exercises can help with lower back pain?\"})[\"response\"].content"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Exercises that can help with lower back pain include:\\n\\n- Cat-Cow Stretch: Start on your hands and knees, then alternate between arching your back up (cat) and letting it sag down (cow). Perform 10-15 repetitions.\\n- Bird Dog: From hands and knees, extend opposite arm and leg while keeping your core engaged. Hold each extension for 5 seconds, then switch sides. Do 10 repetitions per side.\\n- Pelvic Tilts: Lie on your back with knees bent, flatten your lower back against the floor by tightening your abdominal muscles and tilting your pelvis slightly upward. Hold for 10 seconds and repeat 8-12 times.\\n\\nThese exercises are gentle stretching and strengthening movements that can help alleviate and prevent lower back pain.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "igfinyneSQkh",
    "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3",
    "ExecuteTime": {
     "end_time": "2026-02-19T07:21:36.638053Z",
     "start_time": "2026-02-19T07:21:32.994322Z"
    }
   },
   "source": [
    "bm25_retrieval_chain.invoke({\"question\" : \"How does sleep affect overall health?\"})[\"response\"].content"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sleep plays a vital role in overall health. Practicing good sleep hygiene, such as maintaining a consistent sleep schedule, creating a comfortable sleep environment (cool, dark, quiet), and establishing relaxing routines before bed, helps improve sleep quality. Adequate sleep supports immune function, mental health, and vital bodily processes. Conversely, poor sleep or insomnia can negatively impact overall wellness, emphasizing the importance of healthy sleep habits for maintaining good health.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "w0H7pV_USSMQ",
    "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534",
    "ExecuteTime": {
     "end_time": "2026-02-19T07:21:49.456132Z",
     "start_time": "2026-02-19T07:21:47.881461Z"
    }
   },
   "source": [
    "bm25_retrieval_chain.invoke({\"question\" : \"What are some natural remedies for stress and headaches?\"})[\"response\"].content"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some natural remedies for stress and headaches include relaxation techniques such as meditation and deep breathing exercises, progressive muscle relaxation, and herbal teas like chamomile or valerian root. Additionally, addressing triggers such as dehydration, poor sleep, or eye strain can help prevent headaches. For stress, practicing mindfulness and incorporating relaxation methods into your routine can be beneficial.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvg5xHaUdxCl"
   },
   "source": "|It's not clear that this is better or worse, if only we had a way to test this (SPOILERS: We do, the second half of the notebook will cover this)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### â“ Question #1:\n",
    "\n",
    "Give an example query where BM25 is better than embeddings and justify your answer.\n",
    "\n",
    "##### Answer:\n",
    "\n",
    "- BM25 will be better than embeddings (vector search) in scenarios where the input query contains rare words or text that may not be embedded well into any embedding model\n",
    "- IE: say you are a nurse asking a question particular to an ICD-10 code (codes that identify diseases) in your query to the RAG prompt\n",
    "  - since BM25 internally uses `TF-IDF`, it will actually give a high score to chunks of data that contain this ICD-10 code and score it highly because ICD-10 codes are pretty rare\n",
    "  - in an embedding model, the ICD-10 code may not even exist since they are usually very rare (you may need to utilize an embedding model that is specifically trained/used on healthcare/electronic medical record data)\n",
    "  - so anytime a rare keyword is important to an input query or question (any sort of keyword search), BM25 will do much better than an embedding model because the semantic meaning of the query doesn't matter as much as the keywords do\n",
    "  - BM25 will always except at Control-F type of queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-dcbFn2vpZF"
   },
   "source": [
    "## Task 6: Contextual Compression (Using Reranking)\n",
    "\n",
    "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
    "\n",
    "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
    "\n",
    "The basic idea here is this:\n",
    "\n",
    "- We retrieve lots of documents that are very likely related to our query vector\n",
    "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
    "\n",
    "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
    "\n",
    "All we need to do is the following:\n",
    "\n",
    "- Create a basic retriever\n",
    "- Create a compressor (reranker, in this case)\n",
    "\n",
    "That's it!\n",
    "\n",
    "Let's see it in the code below!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "psHvO2K1v_ZQ",
    "ExecuteTime": {
     "end_time": "2026-02-19T07:33:09.973694Z",
     "start_time": "2026-02-19T07:33:08.655452Z"
    }
   },
   "source": [
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank\n",
    "\n",
    "compressor = CohereRerank(model=\"rerank-v3.5\")\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=naive_retriever\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TA9RB2x-j7P"
   },
   "source": [
    "Let's create our chain again, and see how this does!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1BXqmxvHwX6T",
    "ExecuteTime": {
     "end_time": "2026-02-19T07:33:12.647670Z",
     "start_time": "2026-02-19T07:33:12.633797Z"
    }
   },
   "source": [
    "contextual_compression_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "V3iGpokswcBb",
    "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59",
    "ExecuteTime": {
     "end_time": "2026-02-19T07:33:19.517318Z",
     "start_time": "2026-02-19T07:33:15.156831Z"
    }
   },
   "source": [
    "contextual_compression_retrieval_chain.invoke({\"question\" : \"What exercises can help with lower back pain?\"})[\"response\"].content"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided information, exercises that can help with lower back pain include:\\n\\n1. **Cat-Cow Stretch**: Start on your hands and knees. Alternate between arching your back up (like a cat) and letting it sag down (like a cow). Perform 10-15 repetitions.\\n\\n2. **Bird Dog**: On your hands and knees, extend opposite arm and leg while keeping your core engaged. Hold each extension for about 5 seconds, then switch sides. Do 10 repetitions per side.\\n\\n3. **Pelvic Tilts**: Lie on your back with knees bent. Flatten your back against the floor by tightening your abdominal muscles and tilting your pelvis slightly upward. Hold for 10 seconds and repeat 8-12 times.\\n\\nThese gentle stretching and strengthening exercises can help alleviate lower back discomfort and may also prevent future episodes.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "7u_k0i4OweUd",
    "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799",
    "ExecuteTime": {
     "end_time": "2026-02-19T07:33:35.403476Z",
     "start_time": "2026-02-19T07:33:32.837403Z"
    }
   },
   "source": [
    "contextual_compression_retrieval_chain.invoke({\"question\" : \"How does sleep affect overall health?\"})[\"response\"].content"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sleep has a significant impact on overall health. It is essential for physical health, mental well-being, and cognitive function. During sleep, the body repairs tissues, consolidates memories, and releases hormones that regulate growth and appetite. Adequate sleep, typically 7-9 hours per night, supports the body's healing processes, helps maintain a healthy immune system, and enhances mental clarity and emotional stability. Conversely, insufficient or poor-quality sleep can impair these functions and negatively affect overall health.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "zn1EqaGqweXN",
    "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b",
    "ExecuteTime": {
     "end_time": "2026-02-19T07:34:08.973638Z",
     "start_time": "2026-02-19T07:34:06.673907Z"
    }
   },
   "source": [
    "contextual_compression_retrieval_chain.invoke({\"question\" : \"What are some natural remedies for stress and headaches?\"})[\"response\"].content"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some natural remedies for stress and headaches include drinking water to stay hydrated, applying cold or warm compresses to the head or neck, resting in a dark and quiet room, giving gentle massage to the temples and neck, using essential oils like peppermint or lavender, maintaining a regular sleep schedule, practicing deep breathing, engaging in progressive muscle relaxation, using grounding techniques, taking short walks in nature, and listening to calming music.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEbT0g2S-mZ4"
   },
   "source": [
    "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqbghrBEQNn5"
   },
   "source": [
    "## Task 7: Multi-Query Retriever\n",
    "\n",
    "Typically in RAG we have a single query - the one provided by the user.\n",
    "\n",
    "What if we had....more than one query!\n",
    "\n",
    "In essence, a Multi-Query Retriever works by:\n",
    "\n",
    "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
    "2. Retrieving documents for each query.\n",
    "3. Using all unique retrieved documents as context\n",
    "\n",
    "So, how is it to set-up? Not bad! Let's see it down below!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pfM26ReXQjzU",
    "ExecuteTime": {
     "end_time": "2026-02-19T07:34:55.569928Z",
     "start_time": "2026-02-19T07:34:55.543497Z"
    }
   },
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=naive_retriever, llm=chat_model\n",
    ") "
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1vRc129jQ5WW",
    "ExecuteTime": {
     "end_time": "2026-02-19T07:35:06.500778Z",
     "start_time": "2026-02-19T07:35:06.484169Z"
    }
   },
   "source": [
    "multi_query_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "CGgNuOb3Q3M9",
    "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7",
    "ExecuteTime": {
     "end_time": "2026-02-19T07:35:20.121306Z",
     "start_time": "2026-02-19T07:35:13.758981Z"
    }
   },
   "source": [
    "multi_query_retrieval_chain.invoke({\"question\" : \"What exercises can help with lower back pain?\"})[\"response\"].content"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Exercises that can help with lower back pain include:\\n\\n- **Cat-Cow Stretch:** Start on your hands and knees, then alternate between arching your back upward (cat) and letting it sag down (cow). Perform 10-15 repetitions.\\n- **Bird Dog:** From hands and knees, extend opposite arm and leg simultaneously, keeping your core engaged. Hold for 5 seconds, then switch sides. Do 10 repetitions per side.\\n- **Partial Crunches:** Lie on your back with knees bent, arms crossed over your chest, tighten your stomach muscles, and lift your shoulders off the floor. Hold briefly and then lower. Do 8-12 repetitions.\\n- **Knee-to-Chest Stretch:** Lie on your back, pull one knee toward your chest while keeping the other foot flat. Hold for 15-30 seconds, then switch legs.\\n- **Pelvic Tilts:** Lie on your back with knees bent, flatten your back against the floor by tightening your abs and tilting your pelvis upward. Hold for 10 seconds and repeat 8-12 times.\\n\\nThese exercises, when done properly, can help alleviate and prevent lower back pain.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "aAlSthxrRDBC",
    "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb",
    "ExecuteTime": {
     "end_time": "2026-02-19T07:35:27.195070Z",
     "start_time": "2026-02-19T07:35:23.321709Z"
    }
   },
   "source": [
    "multi_query_retrieval_chain.invoke({\"question\" : \"How does sleep affect overall health?\"})[\"response\"].content"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sleep has a significant impact on overall health. According to the information provided, sleep is crucial for physical health, mental well-being, and cognitive function. During sleep, your body repairs tissues, consolidates memories, and releases hormones that regulate growth and appetite. Adequate sleepâ€”typically 7-9 hours for adultsâ€”is essential for maintaining a strong immune system, supporting mental health, and ensuring optimal cognitive performance. Poor sleep or sleep problems like insomnia can negatively affect health, leading to issues such as stress, fatigue, and decreased immunity. Therefore, establishing good sleep hygiene and creating a sleep-friendly environment are vital practices to promote overall health.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "Uv1mpCK8REs4",
    "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369",
    "ExecuteTime": {
     "end_time": "2026-02-19T07:35:37.362511Z",
     "start_time": "2026-02-19T07:35:34.011223Z"
    }
   },
   "source": [
    "multi_query_retrieval_chain.invoke({\"question\" : \"What are some natural remedies for stress and headaches?\"})[\"response\"].content"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some natural remedies for stress and headaches include:\\n\\n- Deep breathing exercises (e.g., inhale for 4 counts, hold for 4, exhale for 4)\\n- Progressive muscle relaxation (tensing and releasing muscle groups)\\n- Grounding techniques (naming things you see, hear, feel, smell, and taste)\\n- Taking short walks, preferably in nature\\n- Listening to calming music\\n- Drinking water to stay hydrated\\n- Applying cold or warm compresses to the head or neck\\n- Resting in a dark, quiet room\\n- Gentle massage of temples and neck\\n- Using essential oils like peppermint or lavender\\n- Maintaining a regular sleep schedule\\n\\nThese approaches can help alleviate stress and headaches naturally without medication.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### â“ Question #2:\n",
    "\n",
    "Explain how generating multiple reformulations of a user query can improve recall.\n",
    "\n",
    "##### Answer:\n",
    "- multiple reformulations of a user query will generate slightly different questions with different wording or even different terms/vocabulary\n",
    "  - the different wording will cast a larger net of documents to retrieve (documents that could have been missed by original query) which could help increase recall\n",
    "- since each query has its own set of documents retrieved, there's a high probability that the union of documents will be larger than what the original query would have retrieved; and as long as these documents are semantically relevant to the original query, it should increase the recall\n",
    "- each reformulated query emphasizes different aspects of the original query which could capture new documents that also emphasize those specific angles (that the original query would not have captured)\n",
    "- generating reformulations of the same question is essentially compensating for the context \"compression\" that naturally occurs with embeddings; the reformulations allow us to search in other semantic neighborhoods that may also have relevant documents to our original query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDEawBf_d_3G"
   },
   "source": [
    "## Task 8: Parent Document Retriever\n",
    "\n",
    "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
    "\n",
    "1. We split the full document into large \"parent\" chunks (e.g. 2000 characters).\n",
    "2. Each parent chunk is further split into smaller \"child\" chunks (e.g. 400 characters).\n",
    "3. The child chunks are stored in a VectorStore, while the parent chunks are stored in an in-memory docstore.\n",
    "4. When we query our Retriever, we do a similarity search comparing our query vector to the child chunks.\n",
    "5. Instead of returning the child chunks, we return their associated parent chunks.\n",
    "\n",
    "The basic idea is:\n",
    "\n",
    "- **Search** for small, focused chunks (better semantic matching)\n",
    "- **Return** big chunks (richer surrounding context)\n",
    "\n",
    "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
    "\n",
    "Let's start by defining our parent and child splitters."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qJ53JJuMd_ZH",
    "ExecuteTime": {
     "end_time": "2026-02-19T08:00:25.930394Z",
     "start_time": "2026-02-19T08:00:25.905850Z"
    }
   },
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOpXfVUH3gL3"
   },
   "source": [
    "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
    "\n",
    "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rzFc-_9HlGQ-",
    "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e",
    "ExecuteTime": {
     "end_time": "2026-02-19T08:01:51.449645Z",
     "start_time": "2026-02-19T08:01:50.891171Z"
    }
   },
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "client = QdrantClient(location=\":memory:\")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"wellness_parent_child\",\n",
    "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
    ")\n",
    "\n",
    "parent_document_vectorstore = QdrantVectorStore(\n",
    "    collection_name=\"wellness_parent_child\", embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sf_g95FA3s6w"
   },
   "source": [
    "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BpWVjPf4fLUp",
    "ExecuteTime": {
     "end_time": "2026-02-19T08:03:08.438789Z",
     "start_time": "2026-02-19T08:03:08.417802Z"
    }
   },
   "source": [
    "store = InMemoryStore()\n",
    "\n",
    "parent_document_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=parent_document_vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoYmSWfE32Zo"
   },
   "source": [
    "By default, this is empty as we haven't added any documents - let's add some now!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iQ2ZzfKigMZc",
    "ExecuteTime": {
     "end_time": "2026-02-19T08:03:35.139704Z",
     "start_time": "2026-02-19T08:03:34.098754Z"
    }
   },
   "source": [
    "parent_document_retriever.add_documents(raw_docs, ids=None)"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bI7Tip1335rE"
   },
   "source": [
    "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Qq_adt2KlSqp",
    "ExecuteTime": {
     "end_time": "2026-02-19T08:04:37.486531Z",
     "start_time": "2026-02-19T08:04:37.467735Z"
    }
   },
   "source": [
    "parent_document_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNolUVQb4Apt"
   },
   "source": [
    "Let's give it a whirl!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "TXB5i89Zly5W",
    "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054",
    "ExecuteTime": {
     "end_time": "2026-02-19T08:05:44.658786Z",
     "start_time": "2026-02-19T08:05:40.796325Z"
    }
   },
   "source": [
    "parent_document_retrieval_chain.invoke({\"question\" : \"What exercises can help with lower back pain?\"})[\"response\"].content"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To help alleviate lower back pain, the following exercises are recommended:\\n\\n- **Cat-Cow Stretch:** Start on hands and knees; arch your back up (cat) and then let it sag down (cow). Repeat 10-15 times.\\n- **Bird Dog:** From hands and knees, extend opposite arm and leg while engaging your core. Hold for 5 seconds, then switch sides. Do 10 repetitions per side.\\n- **Partial Crunches:** Lie on your back with knees bent, cross arms over your chest, tighten your stomach muscles, and lift your shoulders off the floor. Repeat 8-12 times.\\n- **Knee-to-Chest Stretch:** Lie on your back and pull one knee toward your chest, keeping the other foot flat on the floor. Hold for 15-30 seconds, then switch legs.\\n- **Pelvic Tilts:** Lie on your back with knees bent, flatten your back against the floor by tightening your abs and tilting your pelvis slightly upward. Hold for 10 seconds and repeat 8-12 times.\\n\\nThese exercises, when performed gently and regularly, can help reduce lower back pain and prevent future episodes. However, itâ€™s best to consult a healthcare professional before starting any new exercise routine to ensure itâ€™s suitable for your specific condition.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "V5F1T-wNl3cg",
    "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335",
    "ExecuteTime": {
     "end_time": "2026-02-19T08:05:51.609277Z",
     "start_time": "2026-02-19T08:05:47.766862Z"
    }
   },
   "source": "parent_document_retrieval_chain.invoke({\"question\" : \"How does sleep affect overall health?\"})[\"response\"].content",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sleep has a significant impact on overall health. It is crucial for physical health, mental well-being, and cognitive function. During sleep, the body repairs tissues, consolidates memories, and releases hormones that help regulate growth and appetite. Adults typically need 7-9 hours of quality sleep per night in order for these processes to occur effectively. Good sleep hygieneâ€”such as maintaining a consistent sleep schedule, creating a relaxing bedtime routine, and optimizing the sleep environmentâ€”can improve sleep quality. Adequate sleep supports vital bodily functions, enhances mood, boosts immune function, and promotes better mental clarity and learning. Conversely, poor sleep or sleep disturbances can lead to various health issues, including increased risk of chronic conditions, impaired immune response, and mental health challenges.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "ZqARszGzvGcG",
    "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a",
    "ExecuteTime": {
     "end_time": "2026-02-19T08:05:53.754683Z",
     "start_time": "2026-02-19T08:05:51.623614Z"
    }
   },
   "source": [
    "parent_document_retrieval_chain.invoke({\"question\" : \"What are some natural remedies for stress and headaches?\"})[\"response\"].content"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some natural remedies for stress and headaches include:\\n\\n- Deep breathing exercises to promote relaxation\\n- Progressive muscle relaxation to reduce physical tension\\n- Grounding techniques like identifying things you see, hear, feel, smell, and taste to center yourself\\n- Taking short walks, preferably in nature, to clear the mind\\n- Listening to calming music\\n- Using essential oils such as peppermint or lavender, which can help alleviate headache symptoms\\n- Ensuring adequate hydration by drinking plenty of water\\n- Resting in a dark, quiet room to reduce headache symptoms\\n- Gentle massage of the temples and neck to relieve tension\\n- Maintaining a regular sleep schedule and practicing good sleep hygiene\\n\\nThese approaches focus on calming the mind and body naturally, which can help manage stress and headaches effectively.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B41cj42s4DPM"
   },
   "source": [
    "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUrIBKl_TwS9"
   },
   "source": [
    "## Task 9: Ensemble Retriever\n",
    "\n",
    "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
    "\n",
    "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
    "\n",
    "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8j7jpZsKTxic",
    "ExecuteTime": {
     "end_time": "2026-02-19T08:08:49.302754Z",
     "start_time": "2026-02-19T08:08:49.281803Z"
    }
   },
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
    "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=retriever_list, weights=equal_weighting\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpo9Psl5hhJ-"
   },
   "source": [
    "We'll pack *all* of these retrievers together in an ensemble."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KZ__EZwpUKkd",
    "ExecuteTime": {
     "end_time": "2026-02-19T08:08:54.298033Z",
     "start_time": "2026-02-19T08:08:54.279055Z"
    }
   },
   "source": [
    "ensemble_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSsvHpRMj24L"
   },
   "source": [
    "Let's look at our results!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "0lMvqL88UQI-",
    "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889",
    "ExecuteTime": {
     "end_time": "2026-02-19T08:09:11.777819Z",
     "start_time": "2026-02-19T08:09:02.513848Z"
    }
   },
   "source": [
    "ensemble_retrieval_chain.invoke({\"question\" : \"What exercises can help with lower back pain?\"})[\"response\"].content"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Exercises that can help with lower back pain include:\\n\\n- **Cat-Cow Stretch:** Start on your hands and knees, alternate between arching your back up (like a cat) and letting it sag down (like a cow). Perform 10-15 repetitions.\\n\\n- **Bird Dog:** From hands and knees, extend opposite arm and leg while keeping your core engaged. Hold for 5 seconds, then switch sides. Do 10 repetitions per side.\\n\\n- **Pelvic Tilts:** Lie on your back with knees bent, tighten your abs, and tilt your pelvis slightly upward to flatten your back against the floor. Hold for 10 seconds and repeat 8-12 times.\\n\\n- **Partial Crunches:** Lie on your back with knees bent, cross your arms over your chest, and tighten your stomach muscles to lift your shoulders off the floor. Hold briefly, then lower down. Do 8-12 repetitions.\\n\\n- **Knee-to-Chest Stretch:** Lie on your back, pull one knee toward your chest while keeping the other foot flat on the floor. Hold for 15-30 seconds, then switch legs.\\n\\nThese exercises, when done gently and consistently, can help alleviate lower back discomfort and improve flexibility and strength. However, if you have severe pain or any medical concerns, consult a healthcare professional before starting new exercises.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "MNFWLYECURI1",
    "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5",
    "ExecuteTime": {
     "end_time": "2026-02-19T08:09:16.505587Z",
     "start_time": "2026-02-19T08:09:11.778784Z"
    }
   },
   "source": [
    "ensemble_retrieval_chain.invoke({\"question\" : \"How does sleep affect overall health?\"})[\"response\"].content"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sleep plays a vital role in maintaining overall health. It supports physical well-being by allowing the body to repair tissues and regenerate, especially during deep sleep stages. Sleep also enhances mental health by helping consolidate memories and regulate emotions. Additionally, sufficient sleep influences cognitive function, mood, hormonal balance (such as growth and appetite hormones), and immune function. Poor or inadequate sleep can lead to various health problems, including increased stress, weakened immunity, cognitive impairments, and higher risk of chronic conditions. Therefore, maintaining good sleep hygiene and creating an optimal sleep environment are important for overall health and wellness.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "A7qbHfWgUR4c",
    "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e",
    "ExecuteTime": {
     "end_time": "2026-02-19T08:09:21.316543Z",
     "start_time": "2026-02-19T08:09:16.518191Z"
    }
   },
   "source": [
    "ensemble_retrieval_chain.invoke({\"question\" : \"What are some natural remedies for stress and headaches?\"})[\"response\"].content"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some natural remedies for stress and headaches include:\\n\\n- Deep breathing exercises\\n- Progressive muscle relaxation\\n- Grounding techniques (naming sights, sounds, feelings, smells, tastes)\\n- Taking short walks, preferably in nature\\n- Listening to calming music\\n- Applying cold or warm compresses to the head or neck\\n- Resting in a dark, quiet room\\n- Gentle massage of temples and neck\\n- Using peppermint or lavender essential oils\\n- Maintaining a regular sleep schedule\\n- Staying well-hydrated by drinking water\\n- Managing stress through regular exercise, social support, hobbies, and mindfulness practices\\n\\nThese approaches can help relieve stress and headaches naturally and promote overall well-being.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MopbkNJAXVaN"
   },
   "source": [
    "## Task 10: Semantic Chunking\n",
    "\n",
    "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
    "\n",
    "Essentially, Semantic Chunking is implemented by:\n",
    "\n",
    "1. Embedding all sentences in the corpus.\n",
    "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
    "  - `percentile`\n",
    "  - `standard_deviation`\n",
    "  - `interquartile`\n",
    "  - `gradient`\n",
    "3. Each sequence of related sentences is kept as a document!\n",
    "\n",
    "Let's see how to implement this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9ciZbFEldv_"
   },
   "source": [
    "We'll use the `percentile` thresholding method for this example which will:\n",
    "\n",
    "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "66EIEWiEYl5y",
    "ExecuteTime": {
     "end_time": "2026-02-19T08:15:50.069006Z",
     "start_time": "2026-02-19T08:15:50.047436Z"
    }
   },
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "semantic_chunker = SemanticChunker(\n",
    "    embeddings,\n",
    "    breakpoint_threshold_type=\"percentile\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqoKmz12mhRW"
   },
   "source": [
    "Now we can split our documents."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ROcV7o68ZIq7",
    "ExecuteTime": {
     "end_time": "2026-02-19T08:17:16.910992Z",
     "start_time": "2026-02-19T08:17:15.631770Z"
    }
   },
   "source": [
    "semantic_documents = semantic_chunker.split_documents(raw_docs)"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8-LNC-Xmjex"
   },
   "source": [
    "Let's create a new vector store."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "h3sl9QjyZhIe",
    "ExecuteTime": {
     "end_time": "2026-02-19T08:17:25.689340Z",
     "start_time": "2026-02-19T08:17:24.775754Z"
    }
   },
   "source": [
    "semantic_vectorstore = QdrantVectorStore.from_documents(\n",
    "    semantic_documents,\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"wellness_guide_semantic_chunks\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eh_r_-LHmmKn"
   },
   "source": [
    "We'll use naive retrieval for this example."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "odVyDUHwZftc",
    "ExecuteTime": {
     "end_time": "2026-02-19T08:17:28.300329Z",
     "start_time": "2026-02-19T08:17:28.278758Z"
    }
   },
   "source": [
    "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mkeiv_ojmp6G"
   },
   "source": [
    "Finally we can create our classic chain!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xWE_0J0mZveG",
    "ExecuteTime": {
     "end_time": "2026-02-19T08:17:30.199254Z",
     "start_time": "2026-02-19T08:17:30.186420Z"
    }
   },
   "source": [
    "semantic_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5pfjLQ3ms9_"
   },
   "source": [
    "And view the results!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "0lN2j-e4Z0SD",
    "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2",
    "ExecuteTime": {
     "end_time": "2026-02-19T08:17:34.422623Z",
     "start_time": "2026-02-19T08:17:32.541604Z"
    }
   },
   "source": [
    "semantic_retrieval_chain.invoke({\"question\" : \"What exercises can help with lower back pain?\"})[\"response\"].content"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Exercises that can help with lower back pain include:\\n\\n- **Cat-Cow Stretch:** Start on hands and knees, alternate between arching your back up (cat) and sagging it down (cow). Do 10-15 repetitions.\\n- **Partial Crunches:** Lie on your back with knees bent, cross arms over chest, tighten stomach muscles, and lift shoulders off the floor. Perform 8-12 repetitions.\\n- **Knee-to-Chest Stretch:** Lie on your back, pull one knee toward your chest, hold for 15-30 seconds, then switch legs.\\n- **Pelvic Tilts:** Lie on your back with knees bent, flatten your back against the floor by tightening your abs and tilting your pelvis slightly. Hold for 10 seconds, repeat 8-12 times.\\n\\nThese gentle stretching and strengthening exercises are recommended to alleviate lower back discomfort and prevent future episodes.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "xdqfBH1SZ3f9",
    "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155",
    "ExecuteTime": {
     "end_time": "2026-02-19T08:17:39.817336Z",
     "start_time": "2026-02-19T08:17:37.681881Z"
    }
   },
   "source": [
    "semantic_retrieval_chain.invoke({\"question\" : \"How does sleep affect overall health?\"})[\"response\"].content"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sleep plays a vital role in overall health by supporting physical, mental, and cognitive functions. During sleep, the body repairs tissues, regenerates cells, consolidates memories, and releases important hormones that regulate growth and appetite. Adults generally need 7-9 hours of sleep per night, with sleep occurring in cycles that include REM and non-REM stages, each contributing to different restorative processes. Good sleep quality, maintained through proper sleep hygiene practices like a consistent routine, a comfortable environment, and limiting screens before bed, is crucial for maintaining optimal health. Poor sleep can lead to symptoms such as fatigue, headaches, and low energy, and over time, can increase the risk of chronic conditions like obesity, diabetes, cardiovascular disease, and mental health disorders.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "rAcAObZnZ4o6",
    "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a",
    "ExecuteTime": {
     "end_time": "2026-02-19T08:17:41.453748Z",
     "start_time": "2026-02-19T08:17:40.063510Z"
    }
   },
   "source": [
    "semantic_retrieval_chain.invoke({\"question\" : \"What are some natural remedies for stress and headaches?\"})[\"response\"].content"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some natural remedies for stress include practicing deep breathing exercises, progressive muscle relaxation, grounding techniques (such as naming things you see, hear, feel, smell, and taste), taking short walks in nature, and listening to calming music. For headaches, natural remedies include staying hydrated by drinking water, applying cold or warm compresses to the head or neck, resting in a dark, quiet room, gentle massage of the temples and neck, using essential oils like peppermint or lavender, and maintaining a regular sleep schedule.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### â“ Question #3:\n",
    "\n",
    "If sentences are short and highly repetitive (e.g., FAQs), how might semantic chunking behave, and how would you adjust the algorithm?\n",
    "\n",
    "##### Answer:\n",
    "- the problem with short and highly repetitive sentences (like FAQs) is that many consecutive sentences might be very similar to each other (especially in the embedding space) which will make it difficult to find chunk boundaries with semantic chunking\n",
    "  - the distances from similar sentences will be very small, so thresholding types such as percentile, standard deviation, and interquartile will all have issues finding chunk boundaries (there will be too few boundaries)\n",
    "    - percentile considers the all the distances between consecutive sentences and creates a chunk boundary past a certain percentile threshold (IE 95th percentile)\n",
    "      - but with FAQs, all distances will be very similar (close to uniformly distributed) so the 95th percentile might still be very low causing not enough splits\n",
    "    - the standard deviation threshold type calculates the mean and standard deviation of all consecutive sentence distances and creates a chunk boundary when a distance exceeds `mean + k * std dev` (the parameter k controls the sensitivity)\n",
    "      - this is more adaptative to your document but with FAQs, the standard deviation will be low so small variations could lead to over-splitting\n",
    "    - the interquartile threshold type utilizes the middle 50th percentile of distances (`IQR = Q3 - Q1`) and creates a chunk boundary when distances are greater than `Q3 + k * IQR`\n",
    "      - this is similar to the issue with standard deviaton; FAQs will have a very small IQR due to so many similar chunks so the IQR method may be too sensitive and create too many chunk boundaries\n",
    "- I would adjust the algorithm by utilizing the gradient threshold type\n",
    "  - instead of looking at absolute distances, the gradient method looks at the rate of change in distances\n",
    "  - it creates a chunk boundary when there is a sudden change (or rate of change) in distances (it detects jumps in semantic dissimilarity)\n",
    "  - gradient thresholding will perform better than the other threshold types for finding meaningful chunk boundaries and topic changes with repetitive sentences (like FAQs) since it is looking at the rate of change in distance instead of absolute distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xk2n3-pnVWDJ"
   },
   "source": [
    "---\n",
    "\n",
    "# ðŸ¤ Breakout Room Part #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SkJLYwMVZkj"
   },
   "source": [
    "### ðŸ—ï¸ Activity #1:\n",
    "\n",
    "Your task is to evaluate the various Retriever methods against each other.\n",
    "\n",
    "You are expected to:\n",
    "\n",
    "1. Create a \"golden dataset\"\n",
    " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
    "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
    " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparison between them\n",
    "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
    "\n",
    "Your analysis should factor in:\n",
    "  - Cost\n",
    "  - Latency\n",
    "  - Performance\n",
    "\n",
    "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWAr16a5XMub"
   },
   "source": [
    "##### HINTS:\n",
    "\n",
    "- LangSmith provides detailed information about latency and cost."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tgDICngKXLGK",
    "ExecuteTime": {
     "end_time": "2026-02-21T00:39:02.921988Z",
     "start_time": "2026-02-21T00:39:01.097969Z"
    }
   },
   "source": [
    "### YOUR CODE HERE\n",
    "from act1 import run\n",
    "\n",
    "run()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ragas.testset.synthesizers.testset_schema.Testset'>\n",
      "21\n",
      "                                           user_input  \\\n",
      "0   What role do fats play in a balanced diet acco...   \n",
      "1   What role do proteins play in a balanced diet ...   \n",
      "2   What role do minerals play in maintaining a ba...   \n",
      "3                                     What are carbs?   \n",
      "4   What role do carbohydrates play in a balanced ...   \n",
      "5             What minerals are important for health?   \n",
      "6   Can you tell me what PART 2 is about in that w...   \n",
      "7   As a Holistic Wellness Coach, how can establis...   \n",
      "8       What are some natural remedies for headaches?   \n",
      "9   What are some natural ways to support digestiv...   \n",
      "10                    How work-life balance help you?   \n",
      "11  What are some natural ways to support digestiv...   \n",
      "12  How can morning routines help improve your wel...   \n",
      "13  As a Holistic Wellness Coach, how can understa...   \n",
      "14  What are some recommended exercises for reliev...   \n",
      "15  How does the shoulder contribute to overall ph...   \n",
      "16  How can I incorporate the concept of Monday in...   \n",
      "17                 Whaat is the benifits of exercize?   \n",
      "18  What are some recommended exercises to help ma...   \n",
      "19                      Tuesday what I do for health?   \n",
      "20                     Thursday make good for health?   \n",
      "\n",
      "                                   reference_contexts  \\\n",
      "0   [PART 2: NUTRITION AND DIET Chapter 4: Fundame...   \n",
      "1   [PART 2: NUTRITION AND DIET Chapter 4: Fundame...   \n",
      "2   [PART 2: NUTRITION AND DIET Chapter 4: Fundame...   \n",
      "3   [PART 2: NUTRITION AND DIET Chapter 4: Fundame...   \n",
      "4   [PART 2: NUTRITION AND DIET Chapter 4: Fundame...   \n",
      "5   [PART 2: NUTRITION AND DIET Chapter 4: Fundame...   \n",
      "6   [PART 2: NUTRITION AND DIET Chapter 4: Fundame...   \n",
      "7   [13: The Science of Habit Formation Habits are...   \n",
      "8   [13: The Science of Habit Formation Habits are...   \n",
      "9   [13: The Science of Habit Formation Habits are...   \n",
      "10  [13: The Science of Habit Formation Habits are...   \n",
      "11  [13: The Science of Habit Formation Habits are...   \n",
      "12  [13: The Science of Habit Formation Habits are...   \n",
      "13  [13: The Science of Habit Formation Habits are...   \n",
      "14  [The Personal Wellness Guide A Comprehensive R...   \n",
      "15  [The Personal Wellness Guide A Comprehensive R...   \n",
      "16  [The Personal Wellness Guide A Comprehensive R...   \n",
      "17  [The Personal Wellness Guide A Comprehensive R...   \n",
      "18  [The Personal Wellness Guide A Comprehensive R...   \n",
      "19  [The Personal Wellness Guide A Comprehensive R...   \n",
      "20  [The Personal Wellness Guide A Comprehensive R...   \n",
      "\n",
      "                                            reference  \\\n",
      "0   Fats are necessary for hormone production and ...   \n",
      "1   Proteins are essential for muscle repair and i...   \n",
      "2   Minerals are inorganic elements like calcium, ...   \n",
      "3   Carbohydrates are the primary energy source in...   \n",
      "4   Carbohydrates are described as the primary ene...   \n",
      "5   Minerals are inorganic elements like calcium, ...   \n",
      "6   PART 2: NUTRITION AND DIET covers the fundamen...   \n",
      "7   A consistent morning routine, such as waking a...   \n",
      "8   Natural headache remedies include drinking wat...   \n",
      "9   Supporting digestive health involves eating fi...   \n",
      "10  Maintaining work-life balance is essential for...   \n",
      "11  Supporting digestive health involves eating fi...   \n",
      "12  A intentional morning routine can boost produc...   \n",
      "13  According to the provided context, strong soci...   \n",
      "14  The exercises recommended for relieving neck a...   \n",
      "15  The context discusses shoulder tension caused ...   \n",
      "16  The Personal Wellness Guide suggests starting ...   \n",
      "17  Exercise is one of the most important things y...   \n",
      "18  Recommended exercises for lower back pain incl...   \n",
      "19  The context mentions a Tuesday routine involvi...   \n",
      "20  The context emphasizes the importance of regul...   \n",
      "\n",
      "                        synthesizer_name  \n",
      "0   single_hop_specifc_query_synthesizer  \n",
      "1   single_hop_specifc_query_synthesizer  \n",
      "2   single_hop_specifc_query_synthesizer  \n",
      "3   single_hop_specifc_query_synthesizer  \n",
      "4   single_hop_specifc_query_synthesizer  \n",
      "5   single_hop_specifc_query_synthesizer  \n",
      "6   single_hop_specifc_query_synthesizer  \n",
      "7   single_hop_specifc_query_synthesizer  \n",
      "8   single_hop_specifc_query_synthesizer  \n",
      "9   single_hop_specifc_query_synthesizer  \n",
      "10  single_hop_specifc_query_synthesizer  \n",
      "11  single_hop_specifc_query_synthesizer  \n",
      "12  single_hop_specifc_query_synthesizer  \n",
      "13  single_hop_specifc_query_synthesizer  \n",
      "14  single_hop_specifc_query_synthesizer  \n",
      "15  single_hop_specifc_query_synthesizer  \n",
      "16  single_hop_specifc_query_synthesizer  \n",
      "17  single_hop_specifc_query_synthesizer  \n",
      "18  single_hop_specifc_query_synthesizer  \n",
      "19  single_hop_specifc_query_synthesizer  \n",
      "20  single_hop_specifc_query_synthesizer  \n",
      "['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__dataclass_fields__', '__dataclass_params__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__firstlineno__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__match_args__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__post_init__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', 'cost_cb', 'features', 'from_annotated', 'from_dict', 'from_hf_dataset', 'from_jsonl', 'from_list', 'from_pandas', 'get_sample_type', 'run_id', 'samples', 'to_csv', 'to_evaluation_dataset', 'to_hf_dataset', 'to_jsonl', 'to_list', 'to_pandas', 'total_cost', 'total_tokens', 'upload', 'validate_samples']\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-24T21:13:00.935128Z",
     "start_time": "2026-02-24T21:13:00.881485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List, TypedDict, Dict, Callable, Any\n",
    "import pickle\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import logging\n",
    "\n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever, MultiQueryRetriever, ParentDocumentRetriever, \\\n",
    "    EnsembleRetriever\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from ragas import RunConfig, evaluate, EvaluationDataset\n",
    "from ragas.llms import LangchainLLMWrapper, BaseRagasLLM\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper, BaseRagasEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents.base import Document\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas.dataset_schema import EvaluationResult\n",
    "from dotenv import load_dotenv\n",
    "from ragas.testset.graph import KnowledgeGraph, Node, NodeType\n",
    "from ragas.testset.transforms import default_transforms, apply_transforms\n",
    "from ragas.testset.synthesizers import default_query_distribution, SingleHopSpecificQuerySynthesizer, MultiHopAbstractQuerySynthesizer, MultiHopSpecificQuerySynthesizer\n",
    "from ragas.testset.synthesizers.testset_schema import Testset\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy, ContextEntityRecall, \\\n",
    "    NoiseSensitivity\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from langchain_core.retrievers import RetrieverLike, BaseRetriever\n",
    "from langchain_cohere import CohereRerank\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "HEALTH_AND_WELLNESS_GUIDE_PATH = \"data/HealthWellnessGuide.txt\"\n",
    "GOLDEN_DATA_SET_FILE = \"golden_data_set.pkl\"\n",
    "TEST_SET_SIZE = 15\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "PARENT_CHUNK_SIZE = 2000\n",
    "PARENT_CHUNK_OVERLAP = 200\n",
    "CHILD_CHUNK_SIZE = 400\n",
    "CHILD_CHUNK_OVERLAP = 50\n",
    "SLEEP_SECONDS = 1 # to avoid openAI rate-limiting\n",
    "RAG_PROMPT = \"\"\"\n",
    "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
    "\n",
    "If you do not know the answer, or are unsure, say you don't know.\n",
    "\n",
    "### Question\n",
    "{question}\n",
    "\n",
    "### Context\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "RAG_LLM = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "RAG_EMBEDDING_MODEL = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "def create_knowledge_graph(documents: List[Document], llm: BaseRagasLLM, embedding_model: BaseRagasEmbeddings,\n",
    "                           cache_file: str = 'kg.json') -> KnowledgeGraph:\n",
    "    \"\"\"\n",
    "    This function creates a Knowledge Graph (kg) of the documents supplied to it (this kg is utilized to create test queries downstream)\n",
    "    Args:\n",
    "        documents: the list of documents to create a knowledge graph of\n",
    "        llm: the LLM to utilize to create transformations to our kg\n",
    "        embedding_model: the embedding model to use with the LLM\n",
    "        cache_file: file_path/name for the cached KG file (can reload this instead of redoing computation each time on same list of documents)\n",
    "\n",
    "    Returns: KnowledgeGraph Object\n",
    "    \"\"\"\n",
    "    # create knowledge graph\n",
    "    try:\n",
    "        kg = KnowledgeGraph.load(cache_file)\n",
    "    except FileNotFoundError:\n",
    "        kg = KnowledgeGraph()\n",
    "\n",
    "        # add each document as a node in the KG\n",
    "        for doc in documents:\n",
    "            kg.nodes.append(\n",
    "                Node(type=NodeType.DOCUMENT,\n",
    "                     properties={\"page_content\": doc.page_content, \"document_metadata\": doc.metadata}))\n",
    "\n",
    "        # apply transformations to KG (summarization, extracting headlines and themes..)\n",
    "        transforms = default_transforms(documents=documents, llm=llm, embedding_model=embedding_model)\n",
    "        apply_transforms(kg, transforms)\n",
    "        kg.save(cache_file)\n",
    "    return kg\n",
    "\n",
    "\n",
    "def create_golden_data_set(documents: List[Document], llm: str, embedding_model: str,\n",
    "                           use_cache: bool = True) -> Testset:\n",
    "    # create knowledge graph from corpus (utilized for generating queries via LLM generator)\n",
    "    # kg = create_knowledge_graph(documents, llm, embedding_model, cache_file='kg.json')\n",
    "    try:\n",
    "        test_queries = pickle.load(open(GOLDEN_DATA_SET_FILE, 'rb'))\n",
    "    except FileNotFoundError:\n",
    "        llm_wrapper = LangchainLLMWrapper(ChatOpenAI(model=llm))\n",
    "        embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=embedding_model))\n",
    "        generator = TestsetGenerator(llm=llm_wrapper, embedding_model=embeddings)\n",
    "        test_queries = generator.generate_with_langchain_docs(documents, testset_size=TEST_SET_SIZE)\n",
    "        with open(GOLDEN_DATA_SET_FILE, 'wb') as f:\n",
    "            pickle.dump(test_queries, f)\n",
    "\n",
    "\n",
    "    # distribution of queries we want\n",
    "    # single_specific, multi_abstract, multi_specific = query_type_ratios\n",
    "    # query_distribution = [\n",
    "    #     (SingleHopSpecificQuerySynthesizer(llm=llm), single_specific),\n",
    "    #     (MultiHopAbstractQuerySynthesizer(llm=llm), multi_abstract),\n",
    "    #     (MultiHopSpecificQuerySynthesizer(llm=llm), multi_specific),\n",
    "    # ]\n",
    "\n",
    "    return test_queries\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    response: str\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = RAG_PROMPT.format(question=state[\"question\"], context=docs_content)\n",
    "    response = RAG_LLM.invoke(messages)\n",
    "    return {\"response\": response.content}\n",
    "\n",
    "def get_naive_retriever(vector_store: QdrantVectorStore, k: int = 10, **kwargs):\n",
    "    return vector_store.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "def get_bm25_retriever(docs: List[Document], k: int = 10, **kwargs):\n",
    "    return BM25Retriever.from_documents(docs, k=k)\n",
    "\n",
    "def get_cohere_reranker(base_retriever: RetrieverLike, cohere_model: str = 'rerank-v3.5', **kwargs):\n",
    "    compressor = CohereRerank(model=cohere_model)\n",
    "    return ContextualCompressionRetriever(base_compressor=compressor, base_retriever=base_retriever)\n",
    "\n",
    "def get_multiquery_retriever(retriever: BaseRetriever, llm: BaseLanguageModel, **kwargs):\n",
    "    return MultiQueryRetriever.from_llm(retriever=retriever, llm=llm)\n",
    "\n",
    "def get_parent_document_retriever(parent_chunk_size: int, parent_chunk_overlap: int, child_chunk_size: int,\n",
    "                                  child_chunk_overlap: int, qdrant_client: QdrantClient, collection_name: str,\n",
    "                                  documents: List[Document], openai_embedding_model: str = 'text-embedding-3-small',\n",
    "                                  embedding_size: int = 1536, is_semantic: bool = False, **kwargs):\n",
    "    qdrant_client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vector_config=models.VectorConfig(size=embedding_size,\n",
    "        distance=models.DistanceType.COSINE))\n",
    "    parent_document_vector_store = QdrantVectorStore(\n",
    "        collection_name=collection_name,\n",
    "        embedding=OpenAIEmbeddings(model=openai_embedding_model),\n",
    "        client=qdrant_client)\n",
    "\n",
    "    parent_splitter = None if is_semantic else RecursiveCharacterTextSplitter(chunk_size=parent_chunk_size, overlap=parent_chunk_overlap)\n",
    "    # create an InMemoryStore for the parent document retriever\n",
    "    in_memory_store = InMemoryStore()\n",
    "    parent_document_retriever = ParentDocumentRetriever(\n",
    "        vectorstore=parent_document_vector_store,\n",
    "        docstore=in_memory_store,\n",
    "        child_splitter=RecursiveCharacterTextSplitter(chunk_size=child_chunk_size, overlap=child_chunk_overlap),\n",
    "        parent_splitter=parent_splitter)\n",
    "    parent_document_retriever.add_documents(documents)\n",
    "    return parent_document_retriever\n",
    "\n",
    "def load_sources(file_paths: List[str]):\n",
    "    raw_documents = []\n",
    "\n",
    "    # load all sources for RAG\n",
    "    for file_path in file_paths:\n",
    "        loader = TextLoader(file_path)\n",
    "        raw_documents.extend(loader.load())\n",
    "    return raw_documents\n",
    "\n",
    "def get_ensemble_retriever(retrievers: List[RetrieverLike], weighting: List[float] = None, **kwargs):\n",
    "    if weighting is None:\n",
    "        weighting = [1.0 / len(retrievers)] * len(retrievers)\n",
    "    else:\n",
    "        assert (len(weighting) == len(retrievers)), \"Length of weighting must be equal to length of retrievers!\"\n",
    "    return EnsembleRetriever(retrievers=retrievers, weights=weighting)\n",
    "\n",
    "def set_up_vector_db(docs: List[Document], db_url: str = None, embedding_size: int = 1536, is_semantic: bool = False):\n",
    "    # create vector store and add documents\n",
    "    if db_url is None:\n",
    "        db_url = \":memory:\"\n",
    "\n",
    "    if is_semantic:\n",
    "        vector_store = QdrantVectorStore.from_documents(\n",
    "            docs,\n",
    "            RAG_EMBEDDING_MODEL,\n",
    "            location=db_url,\n",
    "            collection_name=\"wellness_semantic\"\n",
    "        )\n",
    "        return None, vector_store\n",
    "    else:\n",
    "        client = QdrantClient(db_url)\n",
    "        client.create_collection(\n",
    "            collection_name=\"wellness\",\n",
    "            vectors_config=VectorParams(size=embedding_size, distance=Distance.COSINE)\n",
    "        )\n",
    "        vector_store = QdrantVectorStore(\n",
    "            client=client,\n",
    "            collection_name=\"wellness\",\n",
    "            embedding=RAG_EMBEDDING_MODEL\n",
    "        )\n",
    "        _ = vector_store.add_documents(docs)\n",
    "        return client, vector_store\n",
    "\n",
    "def retriever_factory(retriever: RetrieverLike):\n",
    "    def retrieve_node(state: State):\n",
    "        retrieved_docs = retriever.invoke(state[\"question\"])\n",
    "        return {\"context\": retrieved_docs}\n",
    "    return retrieve_node\n",
    "\n",
    "def build_rag_graph(retriever: Callable[[State], Dict[str, Any]]):\n",
    "    rag_graph_builder = StateGraph(State).add_sequence([retriever, generate])\n",
    "    rag_graph_builder.add_edge(START, \"retrieve_node\")  # node name from retriever_factory's __name__\n",
    "    rag_graph = rag_graph_builder.compile()\n",
    "    return rag_graph\n",
    "\n",
    "def invoke_test_queries(rag_graph: StateGraph, dataset: Testset):\n",
    "    # invoke each test query from the Testset object\n",
    "    for test_row in dataset:\n",
    "        response = rag_graph.invoke({\"question\": test_row.eval_sample.user_input})\n",
    "        test_row.eval_sample.response = response[\"response\"]\n",
    "        test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
    "        time.sleep(SLEEP_SECONDS)  # to avoid rate limits\n",
    "    return dataset\n",
    "\n",
    "def gather_rag_statistics(data_set: Testset, llm: LangchainLLMWrapper):\n",
    "    custom_run_config = RunConfig(timeout=60)\n",
    "    results: EvaluationResult = evaluate(\n",
    "        dataset=EvaluationDataset.from_pandas(data_set.to_pandas()),\n",
    "        metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness(), ResponseRelevancy(), ContextEntityRecall(), NoiseSensitivity()],\n",
    "        llm=llm,\n",
    "        run_config=custom_run_config,\n",
    "    )\n",
    "    return results\n",
    "\n",
    "def run_experiment_loop(golden_data_set: Testset, retriever_mapping: Dict[str, Callable], retriever_params: dict,\n",
    "                        evaluator_llm: LangchainLLMWrapper, chunk_strategy: str) -> List[Dict]:\n",
    "    rag_metrics = []\n",
    "    for retriever_str, retriever_func in retriever_mapping.items():\n",
    "        logger.warning(f\"Beginning experiment loop: retriever: {retriever_str}, retriever_func: {retriever_func}, \"\n",
    "                       f\"chunk_strategy: {chunk_strategy}\")\n",
    "        retriever_runnable: RetrieverLike = retriever_func(**retriever_params)\n",
    "\n",
    "        # add retrievers (objects) to utilize for ensemble retriever at the end (mutating retriever_params dic)\n",
    "        if retriever_str == \"retriever\":\n",
    "            retriever_params.get(\"retrievers\").append(retriever_runnable)\n",
    "\n",
    "        retriever_node = retriever_factory(retriever=retriever_runnable)\n",
    "        rag_graph = build_rag_graph(retriever=retriever_node)\n",
    "        tested_data_set = invoke_test_queries(rag_graph, deepcopy(golden_data_set))\n",
    "        rag_stats = gather_rag_statistics(tested_data_set, evaluator_llm)\n",
    "        # EvaluationResult.scores is a list of per-row dicts; aggregate to mean per metric for this run\n",
    "        scores_dict = pd.DataFrame(rag_stats.scores).mean(numeric_only=True, skipna=True).to_dict()\n",
    "        rag_metrics.append({'retriever': retriever_str, 'chunking_strategy': f'{chunk_strategy}', **scores_dict})\n",
    "        logger.warning(f\"Done with experiment loop: retriever: {retriever_str}, retriever_func: {retriever_func.__name__}, \"\n",
    "                       f\"chunk_strategy: {chunk_strategy}\")\n",
    "    return rag_metrics\n",
    "\n",
    "def run_experiment(document_sources: List[str]):\n",
    "    raw_corpus = load_sources(document_sources)\n",
    "\n",
    "    # get golden data set for evaluation of retrieval methods\n",
    "    golden_data_set: Testset = create_golden_data_set(raw_corpus, \"gpt-4.1-nano\", \"text-embedding-3-small\")\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "    wellness_docs = text_splitter.split_documents(raw_corpus)\n",
    "    vector_db_client, vector_store = set_up_vector_db(wellness_docs)\n",
    "\n",
    "    naive_retriever = get_naive_retriever(vector_store=vector_store)\n",
    "    non_semantic_params = {\n",
    "        \"vector_store\": vector_store,                       # get_naive_retriever\n",
    "        # \"docs\": wellness_docs,                              # get_bm25_retriever\n",
    "        # \"base_retriever\": naive_retriever,                  # get_cohere_reranker\n",
    "        \"retriever\": naive_retriever,                       # get_multiquery_retriever\n",
    "        \"llm\": ChatOpenAI(model=\"gpt-4.1-nano\"),            # get_multiquery_retriever\n",
    "        # \"parent_chunk_size\": PARENT_CHUNK_SIZE,             # get_parent_document_retriever\n",
    "        # \"parent_chunk_overlap\": PARENT_CHUNK_OVERLAP,       # get_parent_document_retriever\n",
    "        # \"child_chunk_size\": CHILD_CHUNK_SIZE,               # get_parent_document_retriever\n",
    "        # \"child_chunk_overlap\": CHILD_CHUNK_OVERLAP,         # get_parent_document_retriever\n",
    "        # \"qdrant_client\": vector_db_client,                  # get_parent_document_retriever\n",
    "        # \"collection_name\": \"wellness_guide\",                # get_parent_document_retriever\n",
    "        # \"documents\": raw_corpus,                            # get_parent_document_retriever\n",
    "        \"retrievers\": [],                                   # get_ensemble_retriever\n",
    "    }\n",
    "\n",
    "    retriever_mapping = {\n",
    "        \"naive\": get_naive_retriever,\n",
    "        # \"bm25\": get_bm25_retriever,\n",
    "        # \"reranker\": get_cohere_reranker,\n",
    "        \"multiquery\": get_multiquery_retriever,\n",
    "        # \"parent_document\": get_parent_document_retriever,\n",
    "        # \"ensemble\": get_ensemble_retriever,\n",
    "    }\n",
    "\n",
    "    evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-mini\"))\n",
    "\n",
    "    non_semantic_rag_metrics = run_experiment_loop(\n",
    "        golden_data_set, retriever_mapping, non_semantic_params, evaluator_llm, \"non_semantic_chunking\")\n",
    "\n",
    "    # semantic chunking portion of experiment\n",
    "    semantic_chunker = SemanticChunker(RAG_EMBEDDING_MODEL)\n",
    "    semantic_wellness_docs = semantic_chunker.split_documents(wellness_docs)\n",
    "    _, semantic_vector_store = set_up_vector_db(semantic_wellness_docs, is_semantic=True)\n",
    "\n",
    "    naive_semantic_retriever = get_naive_retriever(semantic_vector_store)\n",
    "    semantic_params = {\n",
    "        \"vector_store\": semantic_vector_store,              # get_naive_retriever\n",
    "        # \"docs\": semantic_wellness_docs,                     # get_bm25_retriever\n",
    "        # \"base_retriever\": naive_semantic_retriever,         # get_cohere_reranker\n",
    "        # \"retriever\": naive_semantic_retriever,              # get_multiquery_retriever\n",
    "        # \"llm\": ChatOpenAI(model=\"gpt-4.1-nano\"),            # get_multiquery_retriever\n",
    "        # \"parent_chunk_size\": PARENT_CHUNK_SIZE,             # get_parent_document_retriever\n",
    "        # \"parent_chunk_overlap\": PARENT_CHUNK_OVERLAP,       # get_parent_document_retriever\n",
    "        # \"child_chunk_size\": CHILD_CHUNK_SIZE,               # get_parent_document_retriever\n",
    "        # \"child_chunk_overlap\": CHILD_CHUNK_OVERLAP,         # get_parent_document_retriever\n",
    "        # \"qdrant_client\": vector_db_client,                  # get_parent_document_retriever\n",
    "        # \"collection_name\": \"semantic_wellness_guide\",       # get_parent_document_retriever\n",
    "        # \"documents\": semantic_wellness_docs,                # get_parent_document_retriever\n",
    "        \"retrievers\": [],                                   # get_ensemble_retriever\n",
    "    }\n",
    "\n",
    "    semantic_mapping = {\n",
    "        \"naive\": get_naive_retriever,\n",
    "        # \"bm25\": get_bm25_retriever,\n",
    "        # \"reranker\": get_cohere_reranker,\n",
    "        # \"multiquery\": get_multiquery_retriever,\n",
    "        # \"parent_document\": get_parent_document_retriever,\n",
    "        # \"ensemble\": get_ensemble_retriever,\n",
    "    }\n",
    "\n",
    "    semantic_rag_metrics = run_experiment_loop(\n",
    "        golden_data_set, semantic_mapping, semantic_params, evaluator_llm, \"semantic_chunking\")\n",
    "\n",
    "    # turn metrics into a dataframe for easier analysis and plotting\n",
    "    metrics_df = pd.DataFrame(non_semantic_rag_metrics + semantic_rag_metrics)\n",
    "    return metrics_df\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    metrics_df = run_experiment(document_sources=[f\"{HEALTH_AND_WELLNESS_GUIDE_PATH}\"])"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-24T21:28:19.654400Z",
     "start_time": "2026-02-24T21:13:03.471794Z"
    }
   },
   "cell_type": "code",
   "source": "metrics_df = run_experiment(document_sources=[f\"{HEALTH_AND_WELLNESS_GUIDE_PATH}\"])",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Applying HeadlinesExtractor:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff4aff0a5e294978942d7b503d550b54"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Applying HeadlineSplitter:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c8157f340c2b46e79d2a67c4bacd44ad"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Applying SummaryExtractor:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "60629146e0e845a6b4d4f9eb8db56e75"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Applying CustomNodeFilter:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c008ceeab4724ad8be8b87d55191bb54"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/7 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "576015daf4534774b4864a5ca77104bc"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f29ba4edda040afb662a8bb70950abe"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Generating personas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e82c46289c2e436b99df2dcf9711b5c3"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Generating Scenarios:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dc84cf3bb7a040459295b80e4f369493"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Generating Samples:   0%|          | 0/6 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f33bdc142dd40aa9c801f7772681950"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning experiment loop: retriever: naive, retriever_func: <function get_naive_retriever at 0x16889a5c0>, chunk_strategy: non_semantic_chunking\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Evaluating:   0%|          | 0/36 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "48c9db9728364c74bd8ff5390477e745"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[11]: TimeoutError()\n",
      "Exception raised in Job[23]: TimeoutError()\n",
      "Exception raised in Job[29]: TimeoutError()\n",
      "Exception raised in Job[35]: TimeoutError()\n",
      "Done with experiment loop: retriever: naive, retriever_func: get_naive_retriever, chunk_strategy: non_semantic_chunking\n",
      "Beginning experiment loop: retriever: naive, retriever_func: <function get_naive_retriever at 0x16889a5c0>, chunk_strategy: semantic_chunking\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Evaluating:   0%|          | 0/36 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9937addd479f4d9f96741d74d2f129a8"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[11]: TimeoutError()\n",
      "Exception raised in Job[23]: TimeoutError()\n",
      "Exception raised in Job[35]: TimeoutError()\n",
      "Done with experiment loop: retriever: naive, retriever_func: get_naive_retriever, chunk_strategy: semantic_chunking\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'Dataframe'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[13]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m metrics_df = \u001B[43mrun_experiment\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocument_sources\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mHEALTH_AND_WELLNESS_GUIDE_PATH\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 339\u001B[39m, in \u001B[36mrun_experiment\u001B[39m\u001B[34m(document_sources)\u001B[39m\n\u001B[32m    335\u001B[39m semantic_rag_metrics = run_experiment_loop(\n\u001B[32m    336\u001B[39m     golden_data_set, retriever_mapping, semantic_params, evaluator_llm, \u001B[33m\"\u001B[39m\u001B[33msemantic_chunking\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    338\u001B[39m \u001B[38;5;66;03m# turn metrics into a dataframe for easier analysis and plotting\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m339\u001B[39m metrics_df = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mDataframe\u001B[49m(non_semantic_rag_metrics + semantic_rag_metrics)\n\u001B[32m    340\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m metrics_df\n",
      "\u001B[31mAttributeError\u001B[39m: module 'pandas' has no attribute 'Dataframe'"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-24T22:50:24.985631Z",
     "start_time": "2026-02-24T22:50:24.923661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List, TypedDict, Dict, Callable, Any\n",
    "import pickle\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import logging\n",
    "\n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever, MultiQueryRetriever, ParentDocumentRetriever, \\\n",
    "    EnsembleRetriever\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from ragas import RunConfig, evaluate, EvaluationDataset\n",
    "from ragas.llms import LangchainLLMWrapper, BaseRagasLLM\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper, BaseRagasEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents.base import Document\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas.dataset_schema import EvaluationResult\n",
    "from dotenv import load_dotenv\n",
    "from ragas.testset.graph import KnowledgeGraph, Node, NodeType\n",
    "from ragas.testset.transforms import default_transforms, apply_transforms\n",
    "from ragas.testset.synthesizers import default_query_distribution, SingleHopSpecificQuerySynthesizer, MultiHopAbstractQuerySynthesizer, MultiHopSpecificQuerySynthesizer\n",
    "from ragas.testset.synthesizers.testset_schema import Testset\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy, ContextEntityRecall, \\\n",
    "    NoiseSensitivity\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from langchain_core.retrievers import RetrieverLike, BaseRetriever\n",
    "from langchain_cohere import CohereRerank\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "HEALTH_AND_WELLNESS_GUIDE_PATH = \"data/HealthWellnessGuide.txt\"\n",
    "GOLDEN_DATA_SET_FILE = \"golden_data_set.pkl\"\n",
    "TEST_SET_SIZE = 15\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "PARENT_CHUNK_SIZE = 2000\n",
    "PARENT_CHUNK_OVERLAP = 200\n",
    "CHILD_CHUNK_SIZE = 400\n",
    "CHILD_CHUNK_OVERLAP = 50\n",
    "SLEEP_SECONDS = 1 # to avoid openAI rate-limiting\n",
    "RAG_PROMPT = \"\"\"\n",
    "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
    "\n",
    "If you do not know the answer, or are unsure, say you don't know.\n",
    "\n",
    "### Question\n",
    "{question}\n",
    "\n",
    "### Context\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "RAG_LLM = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "RAG_EMBEDDING_MODEL = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "def create_knowledge_graph(documents: List[Document], llm: BaseRagasLLM, embedding_model: BaseRagasEmbeddings,\n",
    "                           cache_file: str = 'kg.json') -> KnowledgeGraph:\n",
    "    \"\"\"\n",
    "    This function creates a Knowledge Graph (kg) of the documents supplied to it (this kg is utilized to create test queries downstream)\n",
    "    Args:\n",
    "        documents: the list of documents to create a knowledge graph of\n",
    "        llm: the LLM to utilize to create transformations to our kg\n",
    "        embedding_model: the embedding model to use with the LLM\n",
    "        cache_file: file_path/name for the cached KG file (can reload this instead of redoing computation each time on same list of documents)\n",
    "\n",
    "    Returns: KnowledgeGraph Object\n",
    "    \"\"\"\n",
    "    # create knowledge graph\n",
    "    try:\n",
    "        kg = KnowledgeGraph.load(cache_file)\n",
    "    except FileNotFoundError:\n",
    "        kg = KnowledgeGraph()\n",
    "\n",
    "        # add each document as a node in the KG\n",
    "        for doc in documents:\n",
    "            kg.nodes.append(\n",
    "                Node(type=NodeType.DOCUMENT,\n",
    "                     properties={\"page_content\": doc.page_content, \"document_metadata\": doc.metadata}))\n",
    "\n",
    "        # apply transformations to KG (summarization, extracting headlines and themes..)\n",
    "        transforms = default_transforms(documents=documents, llm=llm, embedding_model=embedding_model)\n",
    "        apply_transforms(kg, transforms)\n",
    "        kg.save(cache_file)\n",
    "    return kg\n",
    "\n",
    "\n",
    "def create_golden_data_set(documents: List[Document], llm: str, embedding_model: str,\n",
    "                           use_cache: bool = True) -> Testset:\n",
    "    # create knowledge graph from corpus (utilized for generating queries via LLM generator)\n",
    "    # kg = create_knowledge_graph(documents, llm, embedding_model, cache_file='kg.json')\n",
    "    try:\n",
    "        test_queries = pickle.load(open(GOLDEN_DATA_SET_FILE, 'rb'))\n",
    "    except FileNotFoundError:\n",
    "        llm_wrapper = LangchainLLMWrapper(ChatOpenAI(model=llm))\n",
    "        embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=embedding_model))\n",
    "        generator = TestsetGenerator(llm=llm_wrapper, embedding_model=embeddings)\n",
    "        test_queries = generator.generate_with_langchain_docs(documents, testset_size=TEST_SET_SIZE)\n",
    "        with open(GOLDEN_DATA_SET_FILE, 'wb') as f:\n",
    "            pickle.dump(test_queries, f)\n",
    "\n",
    "\n",
    "    # distribution of queries we want\n",
    "    # single_specific, multi_abstract, multi_specific = query_type_ratios\n",
    "    # query_distribution = [\n",
    "    #     (SingleHopSpecificQuerySynthesizer(llm=llm), single_specific),\n",
    "    #     (MultiHopAbstractQuerySynthesizer(llm=llm), multi_abstract),\n",
    "    #     (MultiHopSpecificQuerySynthesizer(llm=llm), multi_specific),\n",
    "    # ]\n",
    "\n",
    "    return test_queries\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    response: str\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = RAG_PROMPT.format(question=state[\"question\"], context=docs_content)\n",
    "    response = RAG_LLM.invoke(messages)\n",
    "    return {\"response\": response.content}\n",
    "\n",
    "def get_naive_retriever(vector_store: QdrantVectorStore, k: int = 10, **kwargs):\n",
    "    return vector_store.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "def get_bm25_retriever(docs: List[Document], k: int = 10, **kwargs):\n",
    "    return BM25Retriever.from_documents(docs, k=k)\n",
    "\n",
    "def get_cohere_reranker(base_retriever: RetrieverLike, cohere_model: str = 'rerank-v3.5', **kwargs):\n",
    "    compressor = CohereRerank(model=cohere_model)\n",
    "    return ContextualCompressionRetriever(base_compressor=compressor, base_retriever=base_retriever)\n",
    "\n",
    "def get_multiquery_retriever(retriever: BaseRetriever, llm: BaseLanguageModel, **kwargs):\n",
    "    return MultiQueryRetriever.from_llm(retriever=retriever, llm=llm)\n",
    "\n",
    "def get_parent_document_retriever(parent_chunk_size: int, parent_chunk_overlap: int, child_chunk_size: int,\n",
    "                                  child_chunk_overlap: int, qdrant_client: QdrantClient, collection_name: str,\n",
    "                                  documents: List[Document], openai_embedding_model: str = 'text-embedding-3-small',\n",
    "                                  embedding_size: int = 1536, is_semantic: bool = False, **kwargs):\n",
    "    qdrant_client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vector_config=models.VectorConfig(size=embedding_size,\n",
    "        distance=models.DistanceType.COSINE))\n",
    "    parent_document_vector_store = QdrantVectorStore(\n",
    "        collection_name=collection_name,\n",
    "        embedding=OpenAIEmbeddings(model=openai_embedding_model),\n",
    "        client=qdrant_client)\n",
    "\n",
    "    parent_splitter = None if is_semantic else RecursiveCharacterTextSplitter(chunk_size=parent_chunk_size, overlap=parent_chunk_overlap)\n",
    "    # create an InMemoryStore for the parent document retriever\n",
    "    in_memory_store = InMemoryStore()\n",
    "    parent_document_retriever = ParentDocumentRetriever(\n",
    "        vectorstore=parent_document_vector_store,\n",
    "        docstore=in_memory_store,\n",
    "        child_splitter=RecursiveCharacterTextSplitter(chunk_size=child_chunk_size, overlap=child_chunk_overlap),\n",
    "        parent_splitter=parent_splitter)\n",
    "    parent_document_retriever.add_documents(documents)\n",
    "    return parent_document_retriever\n",
    "\n",
    "def load_sources(file_paths: List[str]):\n",
    "    raw_documents = []\n",
    "\n",
    "    # load all sources for RAG\n",
    "    for file_path in file_paths:\n",
    "        loader = TextLoader(file_path)\n",
    "        raw_documents.extend(loader.load())\n",
    "    return raw_documents\n",
    "\n",
    "def get_ensemble_retriever(retrievers: List[RetrieverLike], weighting: List[float] = None, **kwargs):\n",
    "    if weighting is None:\n",
    "        weighting = [1.0 / len(retrievers)] * len(retrievers)\n",
    "    else:\n",
    "        assert (len(weighting) == len(retrievers)), \"Length of weighting must be equal to length of retrievers!\"\n",
    "    return EnsembleRetriever(retrievers=retrievers, weights=weighting)\n",
    "\n",
    "def set_up_vector_db(docs: List[Document], db_url: str = None, embedding_size: int = 1536, is_semantic: bool = False):\n",
    "    # create vector store and add documents\n",
    "    if db_url is None:\n",
    "        db_url = \":memory:\"\n",
    "\n",
    "    if is_semantic:\n",
    "        vector_store = QdrantVectorStore.from_documents(\n",
    "            docs,\n",
    "            RAG_EMBEDDING_MODEL,\n",
    "            location=db_url,\n",
    "            collection_name=\"wellness_semantic\"\n",
    "        )\n",
    "        return None, vector_store\n",
    "    else:\n",
    "        client = QdrantClient(db_url)\n",
    "        client.create_collection(\n",
    "            collection_name=\"wellness\",\n",
    "            vectors_config=VectorParams(size=embedding_size, distance=Distance.COSINE)\n",
    "        )\n",
    "        vector_store = QdrantVectorStore(\n",
    "            client=client,\n",
    "            collection_name=\"wellness\",\n",
    "            embedding=RAG_EMBEDDING_MODEL\n",
    "        )\n",
    "        _ = vector_store.add_documents(docs)\n",
    "        return client, vector_store\n",
    "\n",
    "def retriever_factory(retriever: RetrieverLike):\n",
    "    def retrieve_node(state: State):\n",
    "        retrieved_docs = retriever.invoke(state[\"question\"])\n",
    "        return {\"context\": retrieved_docs}\n",
    "    return retrieve_node\n",
    "\n",
    "def build_rag_graph(retriever: Callable[[State], Dict[str, Any]]):\n",
    "    rag_graph_builder = StateGraph(State).add_sequence([retriever, generate])\n",
    "    rag_graph_builder.add_edge(START, \"retrieve_node\")  # node name from retriever_factory's __name__\n",
    "    rag_graph = rag_graph_builder.compile()\n",
    "    return rag_graph\n",
    "\n",
    "def invoke_test_queries(rag_graph: StateGraph, dataset: Testset):\n",
    "    # invoke each test query from the Testset object\n",
    "    for test_row in dataset:\n",
    "        response = rag_graph.invoke({\"question\": test_row.eval_sample.user_input})\n",
    "        test_row.eval_sample.response = response[\"response\"]\n",
    "        test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
    "        time.sleep(SLEEP_SECONDS)  # to avoid rate limits\n",
    "    return dataset\n",
    "\n",
    "def gather_rag_statistics(data_set: Testset, llm: LangchainLLMWrapper):\n",
    "    custom_run_config = RunConfig(timeout=60)\n",
    "    results: EvaluationResult = evaluate(\n",
    "        dataset=EvaluationDataset.from_pandas(data_set.to_pandas()),\n",
    "        metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness(), ResponseRelevancy(), ContextEntityRecall(), NoiseSensitivity()],\n",
    "        llm=llm,\n",
    "        run_config=custom_run_config,\n",
    "    )\n",
    "    return results\n",
    "\n",
    "def run_experiment_loop(golden_data_set: Testset, retriever_mapping: Dict[str, Callable], retriever_params: dict,\n",
    "                        evaluator_llm: LangchainLLMWrapper, chunk_strategy: str) -> List[Dict]:\n",
    "    rag_metrics = []\n",
    "    for retriever_str, retriever_func in retriever_mapping.items():\n",
    "        logger.warning(f\"Beginning experiment loop: retriever: {retriever_str}, retriever_func: {retriever_func}, \"\n",
    "                       f\"chunk_strategy: {chunk_strategy}\")\n",
    "        retriever_runnable: RetrieverLike = retriever_func(**retriever_params)\n",
    "\n",
    "        # add retrievers (objects) to utilize for ensemble retriever at the end (mutating retriever_params dic)\n",
    "        if retriever_str == \"retriever\":\n",
    "            retriever_params.get(\"retrievers\").append(retriever_runnable)\n",
    "\n",
    "        retriever_node = retriever_factory(retriever=retriever_runnable)\n",
    "        rag_graph = build_rag_graph(retriever=retriever_node)\n",
    "        tested_data_set = invoke_test_queries(rag_graph, deepcopy(golden_data_set))\n",
    "        rag_stats = gather_rag_statistics(tested_data_set, evaluator_llm)\n",
    "        # EvaluationResult.scores is a list of per-row dicts; aggregate to mean per metric for this run\n",
    "        scores_dict = pd.DataFrame(rag_stats.scores).mean(numeric_only=True, skipna=True).to_dict()\n",
    "        rag_metrics.append({'retriever': retriever_str, 'chunking_strategy': f'{chunk_strategy}', **scores_dict})\n",
    "        logger.warning(f\"Done with experiment loop: retriever: {retriever_str}, retriever_func: {retriever_func.__name__}, \"\n",
    "                       f\"chunk_strategy: {chunk_strategy}\")\n",
    "    return rag_metrics\n",
    "\n",
    "def run_experiment(document_sources: List[str]):\n",
    "    raw_corpus = load_sources(document_sources)\n",
    "\n",
    "    # get golden data set for evaluation of retrieval methods\n",
    "    golden_data_set: Testset = create_golden_data_set(raw_corpus, \"gpt-4.1-nano\", \"text-embedding-3-small\")\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "    wellness_docs = text_splitter.split_documents(raw_corpus)\n",
    "    vector_db_client, vector_store = set_up_vector_db(wellness_docs)\n",
    "\n",
    "    naive_retriever = get_naive_retriever(vector_store=vector_store)\n",
    "    non_semantic_params = {\n",
    "        \"vector_store\": vector_store,                       # get_naive_retriever\n",
    "        # \"docs\": wellness_docs,                              # get_bm25_retriever\n",
    "        # \"base_retriever\": naive_retriever,                  # get_cohere_reranker\n",
    "        \"retriever\": naive_retriever,                       # get_multiquery_retriever\n",
    "        \"llm\": ChatOpenAI(model=\"gpt-4.1-nano\"),            # get_multiquery_retriever\n",
    "        # \"parent_chunk_size\": PARENT_CHUNK_SIZE,             # get_parent_document_retriever\n",
    "        # \"parent_chunk_overlap\": PARENT_CHUNK_OVERLAP,       # get_parent_document_retriever\n",
    "        # \"child_chunk_size\": CHILD_CHUNK_SIZE,               # get_parent_document_retriever\n",
    "        # \"child_chunk_overlap\": CHILD_CHUNK_OVERLAP,         # get_parent_document_retriever\n",
    "        # \"qdrant_client\": vector_db_client,                  # get_parent_document_retriever\n",
    "        # \"collection_name\": \"wellness_guide\",                # get_parent_document_retriever\n",
    "        # \"documents\": raw_corpus,                            # get_parent_document_retriever\n",
    "        \"retrievers\": [],                                   # get_ensemble_retriever\n",
    "    }\n",
    "\n",
    "    retriever_mapping = {\n",
    "        \"naive\": get_naive_retriever,\n",
    "        # \"bm25\": get_bm25_retriever,\n",
    "        # \"reranker\": get_cohere_reranker,\n",
    "        \"multiquery\": get_multiquery_retriever,\n",
    "        # \"parent_document\": get_parent_document_retriever,\n",
    "        # \"ensemble\": get_ensemble_retriever,\n",
    "    }\n",
    "\n",
    "    evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-mini\"))\n",
    "\n",
    "    non_semantic_rag_metrics = run_experiment_loop(\n",
    "        golden_data_set, retriever_mapping, non_semantic_params, evaluator_llm, \"non_semantic_chunking\")\n",
    "\n",
    "    # semantic chunking portion of experiment\n",
    "    semantic_chunker = SemanticChunker(RAG_EMBEDDING_MODEL)\n",
    "    semantic_wellness_docs = semantic_chunker.split_documents(wellness_docs)\n",
    "    _, semantic_vector_store = set_up_vector_db(semantic_wellness_docs, is_semantic=True)\n",
    "\n",
    "    naive_semantic_retriever = get_naive_retriever(semantic_vector_store)\n",
    "    semantic_params = {\n",
    "        \"vector_store\": semantic_vector_store,              # get_naive_retriever\n",
    "        # \"docs\": semantic_wellness_docs,                     # get_bm25_retriever\n",
    "        # \"base_retriever\": naive_semantic_retriever,         # get_cohere_reranker\n",
    "        # \"retriever\": naive_semantic_retriever,              # get_multiquery_retriever\n",
    "        # \"llm\": ChatOpenAI(model=\"gpt-4.1-nano\"),            # get_multiquery_retriever\n",
    "        # \"parent_chunk_size\": PARENT_CHUNK_SIZE,             # get_parent_document_retriever\n",
    "        # \"parent_chunk_overlap\": PARENT_CHUNK_OVERLAP,       # get_parent_document_retriever\n",
    "        # \"child_chunk_size\": CHILD_CHUNK_SIZE,               # get_parent_document_retriever\n",
    "        # \"child_chunk_overlap\": CHILD_CHUNK_OVERLAP,         # get_parent_document_retriever\n",
    "        # \"qdrant_client\": vector_db_client,                  # get_parent_document_retriever\n",
    "        # \"collection_name\": \"semantic_wellness_guide\",       # get_parent_document_retriever\n",
    "        # \"documents\": semantic_wellness_docs,                # get_parent_document_retriever\n",
    "        \"retrievers\": [],                                   # get_ensemble_retriever\n",
    "    }\n",
    "\n",
    "    semantic_mapping = {\n",
    "        \"naive\": get_naive_retriever,\n",
    "        # \"bm25\": get_bm25_retriever,\n",
    "        # \"reranker\": get_cohere_reranker,\n",
    "        # \"multiquery\": get_multiquery_retriever,\n",
    "        # \"parent_document\": get_parent_document_retriever,\n",
    "        # \"ensemble\": get_ensemble_retriever,\n",
    "    }\n",
    "\n",
    "    semantic_rag_metrics = run_experiment_loop(\n",
    "        golden_data_set, semantic_mapping, semantic_params, evaluator_llm, \"semantic_chunking\")\n",
    "\n",
    "    # turn metrics into a dataframe for easier analysis and plotting\n",
    "    metrics_df = pd.DataFrame(non_semantic_rag_metrics + semantic_rag_metrics)\n",
    "    return metrics_df"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-24T22:16:05.967021Z",
     "start_time": "2026-02-24T22:08:47.521532Z"
    }
   },
   "cell_type": "code",
   "source": "metrics_df = run_experiment(document_sources=[f\"{HEALTH_AND_WELLNESS_GUIDE_PATH}\"])",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning experiment loop: retriever: naive, retriever_func: <function get_naive_retriever at 0x14c82cb80>, chunk_strategy: non_semantic_chunking\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Evaluating:   0%|          | 0/36 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "898cd85a72f6478481354e36cbc79561"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[5]: TimeoutError()\n",
      "Exception raised in Job[11]: TimeoutError()\n",
      "Exception raised in Job[17]: TimeoutError()\n",
      "Exception raised in Job[19]: TimeoutError()\n",
      "Exception raised in Job[20]: TimeoutError()\n",
      "Exception raised in Job[23]: TimeoutError()\n",
      "Exception raised in Job[29]: TimeoutError()\n",
      "Exception raised in Job[31]: TimeoutError()\n",
      "Exception raised in Job[32]: TimeoutError()\n",
      "Exception raised in Job[35]: TimeoutError()\n",
      "Done with experiment loop: retriever: naive, retriever_func: get_naive_retriever, chunk_strategy: non_semantic_chunking\n",
      "Beginning experiment loop: retriever: multiquery, retriever_func: <function get_multiquery_retriever at 0x16889afc0>, chunk_strategy: non_semantic_chunking\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Evaluating:   0%|          | 0/36 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "636264fa689e41899d4d3fc452cada8a"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[5]: TimeoutError()\n",
      "Exception raised in Job[11]: TimeoutError()\n",
      "Exception raised in Job[17]: TimeoutError()\n",
      "Exception raised in Job[23]: TimeoutError()\n",
      "Exception raised in Job[25]: TimeoutError()\n",
      "Exception raised in Job[29]: TimeoutError()\n",
      "Exception raised in Job[32]: TimeoutError()\n",
      "Exception raised in Job[35]: TimeoutError()\n",
      "Done with experiment loop: retriever: multiquery, retriever_func: get_multiquery_retriever, chunk_strategy: non_semantic_chunking\n",
      "Beginning experiment loop: retriever: naive, retriever_func: <function get_naive_retriever at 0x14c82cb80>, chunk_strategy: semantic_chunking\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Evaluating:   0%|          | 0/36 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "85796a82887c4edcb87aed4105b9b89e"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[5]: TimeoutError()\n",
      "Exception raised in Job[11]: TimeoutError()\n",
      "Exception raised in Job[17]: TimeoutError()\n",
      "Exception raised in Job[19]: TimeoutError()\n",
      "Exception raised in Job[23]: TimeoutError()\n",
      "Exception raised in Job[29]: TimeoutError()\n",
      "Exception raised in Job[32]: TimeoutError()\n",
      "Exception raised in Job[35]: TimeoutError()\n",
      "Done with experiment loop: retriever: naive, retriever_func: get_naive_retriever, chunk_strategy: semantic_chunking\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-24T22:16:20.027817Z",
     "start_time": "2026-02-24T22:16:20.000124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# this run was a trial run with just a test set size of 5\n",
    "metrics_df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    retriever      chunking_strategy  context_recall  faithfulness  \\\n",
       "0       naive  non_semantic_chunking        0.888889      0.821918   \n",
       "1  multiquery  non_semantic_chunking        0.944444      0.738017   \n",
       "2       naive      semantic_chunking        0.777778      0.758282   \n",
       "\n",
       "   factual_correctness  answer_relevancy  context_entity_recall  \\\n",
       "0               0.4975          0.962616               0.369298   \n",
       "1               0.5000          0.971287               0.341082   \n",
       "2               0.4660          0.973520               0.337427   \n",
       "\n",
       "   noise_sensitivity_relevant  \n",
       "0                         NaN  \n",
       "1                         NaN  \n",
       "2                         NaN  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retriever</th>\n",
       "      <th>chunking_strategy</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>factual_correctness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_entity_recall</th>\n",
       "      <th>noise_sensitivity_relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>naive</td>\n",
       "      <td>non_semantic_chunking</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.821918</td>\n",
       "      <td>0.4975</td>\n",
       "      <td>0.962616</td>\n",
       "      <td>0.369298</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>multiquery</td>\n",
       "      <td>non_semantic_chunking</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.738017</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.971287</td>\n",
       "      <td>0.341082</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>naive</td>\n",
       "      <td>semantic_chunking</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.758282</td>\n",
       "      <td>0.4660</td>\n",
       "      <td>0.973520</td>\n",
       "      <td>0.337427</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- this was a test run with just a test set of 5 (so I feel the numbers are a bit noisy)\n",
    "- let's try with a test set size of **15** which is more statistically sound"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-24T22:34:06.760357Z",
     "start_time": "2026-02-24T22:21:18.845700Z"
    }
   },
   "cell_type": "code",
   "source": "metrics_df2 = run_experiment(document_sources=[f\"{HEALTH_AND_WELLNESS_GUIDE_PATH}\"])",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Applying HeadlinesExtractor:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4ecc98b184bf4acb84aa03fa49068158"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Applying HeadlineSplitter:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ba68d9621da498c9c9ee22c570eaa12"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Applying SummaryExtractor:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f159103ddeb4c68b58ff7694c1f9a15"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Applying CustomNodeFilter:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5146ca7e904847a6b9c049eb5ce9bec7"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/7 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "38aef5580e354d5586c0803e98067000"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "422b24bf74134d9f9942f532bf500551"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Generating personas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b48088bc71474a8eb8ac3e9a6be4de96"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Generating Scenarios:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b29113726b024c46aa90f9fa08d09679"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Generating Samples:   0%|          | 0/15 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2222fb84b86f4b66873b1bb556f3720a"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning experiment loop: retriever: naive, retriever_func: <function get_naive_retriever at 0x14c82cb80>, chunk_strategy: non_semantic_chunking\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d792035867de48728b423c5990de4e29"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[5]: TimeoutError()\n",
      "Exception raised in Job[8]: TimeoutError()\n",
      "Exception raised in Job[11]: TimeoutError()\n",
      "Exception raised in Job[17]: TimeoutError()\n",
      "Exception raised in Job[23]: TimeoutError()\n",
      "Exception raised in Job[29]: TimeoutError()\n",
      "Exception raised in Job[35]: TimeoutError()\n",
      "Exception raised in Job[41]: TimeoutError()\n",
      "Exception raised in Job[47]: TimeoutError()\n",
      "Exception raised in Job[53]: TimeoutError()\n",
      "Exception raised in Job[59]: TimeoutError()\n",
      "Exception raised in Job[65]: TimeoutError()\n",
      "Exception raised in Job[71]: TimeoutError()\n",
      "Exception raised in Job[77]: TimeoutError()\n",
      "Exception raised in Job[83]: TimeoutError()\n",
      "Exception raised in Job[89]: TimeoutError()\n",
      "Done with experiment loop: retriever: naive, retriever_func: get_naive_retriever, chunk_strategy: non_semantic_chunking\n",
      "Beginning experiment loop: retriever: multiquery, retriever_func: <function get_multiquery_retriever at 0x16889afc0>, chunk_strategy: non_semantic_chunking\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eb2a2682eade42c48fac8124319015c0"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[5]: TimeoutError()\n",
      "Exception raised in Job[7]: TimeoutError()\n",
      "Exception raised in Job[11]: TimeoutError()\n",
      "Exception raised in Job[17]: TimeoutError()\n",
      "Exception raised in Job[23]: TimeoutError()\n",
      "Exception raised in Job[29]: TimeoutError()\n",
      "Exception raised in Job[35]: TimeoutError()\n",
      "Exception raised in Job[41]: TimeoutError()\n",
      "Exception raised in Job[47]: TimeoutError()\n",
      "Exception raised in Job[53]: TimeoutError()\n",
      "Exception raised in Job[59]: TimeoutError()\n",
      "Exception raised in Job[65]: TimeoutError()\n",
      "Exception raised in Job[71]: TimeoutError()\n",
      "Exception raised in Job[73]: TimeoutError()\n",
      "Exception raised in Job[77]: TimeoutError()\n",
      "Exception raised in Job[83]: TimeoutError()\n",
      "Exception raised in Job[89]: TimeoutError()\n",
      "Done with experiment loop: retriever: multiquery, retriever_func: get_multiquery_retriever, chunk_strategy: non_semantic_chunking\n",
      "Beginning experiment loop: retriever: naive, retriever_func: <function get_naive_retriever at 0x14c82cb80>, chunk_strategy: semantic_chunking\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e94402f94a374b9b851adbf77c8353bd"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[5]: TimeoutError()\n",
      "Exception raised in Job[11]: TimeoutError()\n",
      "Exception raised in Job[17]: TimeoutError()\n",
      "Exception raised in Job[23]: TimeoutError()\n",
      "Exception raised in Job[29]: TimeoutError()\n",
      "Exception raised in Job[35]: TimeoutError()\n",
      "Exception raised in Job[41]: TimeoutError()\n",
      "Exception raised in Job[47]: TimeoutError()\n",
      "Exception raised in Job[53]: TimeoutError()\n",
      "Exception raised in Job[59]: TimeoutError()\n",
      "Exception raised in Job[65]: TimeoutError()\n",
      "Exception raised in Job[71]: TimeoutError()\n",
      "Exception raised in Job[73]: TimeoutError()\n",
      "Exception raised in Job[74]: TimeoutError()\n",
      "Exception raised in Job[77]: TimeoutError()\n",
      "Exception raised in Job[83]: TimeoutError()\n",
      "Exception raised in Job[89]: TimeoutError()\n",
      "Done with experiment loop: retriever: naive, retriever_func: get_naive_retriever, chunk_strategy: semantic_chunking\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-24T22:34:21.741456Z",
     "start_time": "2026-02-24T22:34:21.712544Z"
    }
   },
   "cell_type": "code",
   "source": "metrics_df2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    retriever      chunking_strategy  context_recall  faithfulness  \\\n",
       "0       naive  non_semantic_chunking        0.877778      0.850064   \n",
       "1  multiquery  non_semantic_chunking        0.922222      0.886434   \n",
       "2       naive      semantic_chunking        0.844444      0.859176   \n",
       "\n",
       "   factual_correctness  answer_relevancy  context_entity_recall  \\\n",
       "0             0.520714          0.836143               0.288691   \n",
       "1             0.544000          0.901995               0.306325   \n",
       "2             0.620714          0.974841               0.324565   \n",
       "\n",
       "   noise_sensitivity_relevant  \n",
       "0                         NaN  \n",
       "1                         NaN  \n",
       "2                         NaN  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retriever</th>\n",
       "      <th>chunking_strategy</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>factual_correctness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_entity_recall</th>\n",
       "      <th>noise_sensitivity_relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>naive</td>\n",
       "      <td>non_semantic_chunking</td>\n",
       "      <td>0.877778</td>\n",
       "      <td>0.850064</td>\n",
       "      <td>0.520714</td>\n",
       "      <td>0.836143</td>\n",
       "      <td>0.288691</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>multiquery</td>\n",
       "      <td>non_semantic_chunking</td>\n",
       "      <td>0.922222</td>\n",
       "      <td>0.886434</td>\n",
       "      <td>0.544000</td>\n",
       "      <td>0.901995</td>\n",
       "      <td>0.306325</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>naive</td>\n",
       "      <td>semantic_chunking</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.859176</td>\n",
       "      <td>0.620714</td>\n",
       "      <td>0.974841</td>\n",
       "      <td>0.324565</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-24T23:03:29.085268Z",
     "start_time": "2026-02-24T22:50:44.748096Z"
    }
   },
   "cell_type": "code",
   "source": "metrics_df3 = run_experiment(document_sources=[f\"{HEALTH_AND_WELLNESS_GUIDE_PATH}\"])",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning experiment loop: retriever: naive, retriever_func: <function get_naive_retriever at 0x16ace00e0>, chunk_strategy: non_semantic_chunking\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f62cf3c683f64f6ebc5efa819f6f781d"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[5]: TimeoutError()\n",
      "Exception raised in Job[11]: TimeoutError()\n",
      "Exception raised in Job[17]: TimeoutError()\n",
      "Exception raised in Job[23]: TimeoutError()\n",
      "Exception raised in Job[29]: TimeoutError()\n",
      "Exception raised in Job[35]: TimeoutError()\n",
      "Exception raised in Job[41]: TimeoutError()\n",
      "Exception raised in Job[47]: TimeoutError()\n",
      "Exception raised in Job[53]: TimeoutError()\n",
      "Exception raised in Job[59]: TimeoutError()\n",
      "Exception raised in Job[65]: TimeoutError()\n",
      "Exception raised in Job[71]: TimeoutError()\n",
      "Exception raised in Job[77]: TimeoutError()\n",
      "Exception raised in Job[83]: TimeoutError()\n",
      "Exception raised in Job[89]: TimeoutError()\n",
      "Done with experiment loop: retriever: naive, retriever_func: get_naive_retriever, chunk_strategy: non_semantic_chunking\n",
      "Beginning experiment loop: retriever: multiquery, retriever_func: <function get_multiquery_retriever at 0x16ace0680>, chunk_strategy: non_semantic_chunking\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fbe7793172d5405d9ebbfd86f58f3323"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[5]: TimeoutError()\n",
      "Exception raised in Job[7]: TimeoutError()\n",
      "Exception raised in Job[8]: TimeoutError()\n",
      "Exception raised in Job[11]: TimeoutError()\n",
      "Exception raised in Job[17]: TimeoutError()\n",
      "Exception raised in Job[23]: TimeoutError()\n",
      "Exception raised in Job[29]: TimeoutError()\n",
      "Exception raised in Job[35]: TimeoutError()\n",
      "Exception raised in Job[40]: TimeoutError()\n",
      "Exception raised in Job[41]: TimeoutError()\n",
      "Exception raised in Job[47]: TimeoutError()\n",
      "Exception raised in Job[53]: TimeoutError()\n",
      "Exception raised in Job[59]: TimeoutError()\n",
      "Exception raised in Job[65]: TimeoutError()\n",
      "Exception raised in Job[71]: TimeoutError()\n",
      "Exception raised in Job[77]: TimeoutError()\n",
      "Exception raised in Job[83]: TimeoutError()\n",
      "Exception raised in Job[89]: TimeoutError()\n",
      "Done with experiment loop: retriever: multiquery, retriever_func: get_multiquery_retriever, chunk_strategy: non_semantic_chunking\n",
      "Beginning experiment loop: retriever: naive, retriever_func: <function get_naive_retriever at 0x16ace00e0>, chunk_strategy: semantic_chunking\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "268af566e08142328027d1f54a84c9c1"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[5]: TimeoutError()\n",
      "Exception raised in Job[11]: TimeoutError()\n",
      "Exception raised in Job[17]: TimeoutError()\n",
      "Exception raised in Job[23]: TimeoutError()\n",
      "Exception raised in Job[29]: TimeoutError()\n",
      "Exception raised in Job[35]: TimeoutError()\n",
      "Exception raised in Job[41]: TimeoutError()\n",
      "Exception raised in Job[47]: TimeoutError()\n",
      "Exception raised in Job[53]: TimeoutError()\n",
      "Exception raised in Job[59]: TimeoutError()\n",
      "Exception raised in Job[65]: TimeoutError()\n",
      "Exception raised in Job[71]: TimeoutError()\n",
      "Exception raised in Job[73]: TimeoutError()\n",
      "Exception raised in Job[77]: TimeoutError()\n",
      "Exception raised in Job[83]: TimeoutError()\n",
      "Exception raised in Job[89]: TimeoutError()\n",
      "Done with experiment loop: retriever: naive, retriever_func: get_naive_retriever, chunk_strategy: semantic_chunking\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-24T23:12:26.707026Z",
     "start_time": "2026-02-24T23:12:26.670896Z"
    }
   },
   "cell_type": "code",
   "source": "metrics_df3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    retriever      chunking_strategy  context_recall  faithfulness  \\\n",
       "0       naive  non_semantic_chunking        0.877778      0.867838   \n",
       "1  multiquery  non_semantic_chunking        0.900000      0.839957   \n",
       "2       naive      semantic_chunking        0.827778      0.880908   \n",
       "\n",
       "   factual_correctness  answer_relevancy  context_entity_recall  \\\n",
       "0             0.555333          0.833617               0.288030   \n",
       "1             0.502857          0.902770               0.297127   \n",
       "2             0.533333          0.960202               0.306648   \n",
       "\n",
       "   noise_sensitivity_relevant  \n",
       "0                         NaN  \n",
       "1                         NaN  \n",
       "2                         NaN  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retriever</th>\n",
       "      <th>chunking_strategy</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>factual_correctness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_entity_recall</th>\n",
       "      <th>noise_sensitivity_relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>naive</td>\n",
       "      <td>non_semantic_chunking</td>\n",
       "      <td>0.877778</td>\n",
       "      <td>0.867838</td>\n",
       "      <td>0.555333</td>\n",
       "      <td>0.833617</td>\n",
       "      <td>0.288030</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>multiquery</td>\n",
       "      <td>non_semantic_chunking</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.839957</td>\n",
       "      <td>0.502857</td>\n",
       "      <td>0.902770</td>\n",
       "      <td>0.297127</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>naive</td>\n",
       "      <td>semantic_chunking</td>\n",
       "      <td>0.827778</td>\n",
       "      <td>0.880908</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.960202</td>\n",
       "      <td>0.306648</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " **Multi-Query Retriever Performance**\n",
    "  - Multi-query achieved the highest context recall (0.90), showing it successfully retrieved more relevant information by generating multiple query variations\n",
    "  - However, it had the lowest factual correctness (0.503), suggesting that casting a wider net retrieved more context but potentially introduced less precise information\n",
    "  - Strong answer relevancy (0.903) indicates the retrieved context was still relevant to answering the questions\n",
    "  - multi-query retriever definitely requires more performance considerations (since you have to generate more query versions from an LLM)\n",
    "    - so testing this method comes at a cost of price & latency (price due to more LLM API calls and latency due to need time to generate the other query formulations)\n",
    "\n",
    " **Semantic Chunking**\n",
    "  - Semantic chunking with naive retrieval showed the highest answer relevancy (0.960) and best faithfulness (0.881), meaning answers were more relevant and grounded in the retrieved context\n",
    "  - However, it had the lowest context recall (0.828), suggesting semantic boundaries created chunks that were missed during retrieval\n",
    "  - Best context entity recall (0.307) shows semantic chunking better preserved entity relationships within chunks\n",
    "\n",
    "**Trade-offs**\n",
    "  - Recall vs. Precision trade-off: Multi-query gets more context (high recall) but lower factual correctness, while naive retrieval is more precise but retrieves less\n",
    "  - Chunking strategy matters: Semantic chunking improved answer quality metrics (relevancy, faithfulness) but at the cost of recall\n",
    "  - no method by itself was an obvious winner; each approach was better for different metrics which reinforces the idea that you have to test different retrieval strategies for your own RAG application and its unique data sources\n",
    "\n",
    "**Takeaways**\n",
    "  - If your use case requires comprehensive information retrieval, multi-query is superior\n",
    "  - If answer accuracy and relevance are critical, semantic chunking with proper retrieval tuning may be better\n",
    "  - The \"best\" retriever depends on whether you optimize for recall, precision, or answer quality\n",
    "  - it would be interesting to test semantic chunking with multiquery retrieval to see how the retrieval metrics would have compared (that would be my next experiment)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
